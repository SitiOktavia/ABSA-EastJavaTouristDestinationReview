# -*- coding: utf-8 -*-
"""skripsi 1.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iLb2kQuF2PdziE9HpiKPhmfn2-LjdxUQ

# Library Preparation
"""

!pip install indoNLP
!pip install Singkatan
!pip install gensim
!pip install Sastrawi
!pip install keras_tuner
!pip install transformers
!pip install tensorflow

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam

"""# Dataset"""

import pandas as pd

#file_url='https://drive.google.com/uc?id=1MjeFaOVzEPf9rn-0XRVf5kAgBvIQdTyn'
#file_url = "https://drive.google.com/uc?id=1ojTxP5QhS9wTe1NmBFTLVlsX5eSPFis1"
file_url = "https://drive.google.com/uc?id=1gYolYDxGfmvacy1tKZLIoYmVEi6wdPH7"
df=pd.read_csv(file_url, sep=',')
df

import multilabel_oversampling as mo

# Set random seed untuk reprodusibilitas
mo.seed_everything(42)

# Inisialisasi oversampler
ml_oversampler = mo.MultilabelOversampler(
    number_of_adds=500,  # Jumlah sampel yang akan ditambahkan
    number_of_tries=100  # Maksimal percobaan per iterasi
)

# Jalankan oversampling
df_balanced = ml_oversampler.fit(df)

# Hasil:
print(f"Original size: {len(df)} | Balanced size: {len(df_balanced)}")
print("Original distribution:", ml_oversampler.original_distribution)
print("Balanced distribution:", ml_oversampler.new_distribution)

# Plot hasil
ml_oversampler.plot_all_tries()

"""#EDA

## info
"""

print(df.info())

"""##Lokasi"""

# Distribusi lokasi wisata
plt.figure(figsize=(8,4))
df['location'].value_counts().head(10).plot(kind='barh', color='teal')
plt.title('10 Lokasi Wisata Terbanyak diulas')
plt.xlabel('Jumlah Ulasan')
plt.ylabel('Lokasi')
plt.show()

"""## Sentiment per aspect"""

import matplotlib.pyplot as plt

aspects = ['attraction', 'amenities', 'access', 'price']

for aspect in aspects:
  plt.figure(figsize=(10, 6))  # Adjust figure size as needed
  sns.countplot(x=aspect, data=df)
  plt.title(f'Distribution of {aspect}')
  plt.xlabel(aspect)
  plt.ylabel('Count')
  plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability
  plt.tight_layout() # Adjust layout to prevent labels from overlapping
  plt.show()

import pandas as pd

def count_sentiment(df, column):
    positive = df[df[column] == 'positive'].shape[0]
    negative = df[df[column] == 'negative'].shape[0]
    none = df[df[column] == 'none'].shape[0]
    total = positive + negative + none
    return positive, negative, none, total

# Hitung dan simpan dalam list of dict
columns = ['attraction', 'amenities', 'access', 'price']
summary = []

for col in columns:
    pos, neg, non, tot = count_sentiment(df, col)
    summary.append({
        'Aspect': col,
        'Positive': pos,
        'Negative': neg,
        'None': non,
        'Total': tot
    })

# Ubah ke DataFrame
sentiment_df = pd.DataFrame(summary)

# Tambahkan baris total
total_row = {
    'Aspect': 'Total',
    'Positive': sentiment_df['Positive'].sum(),
    'Negative': sentiment_df['Negative'].sum(),
    'None': sentiment_df['None'].sum(),
    'Total': sentiment_df['Total'].sum()
}
sentiment_df = pd.concat([sentiment_df, pd.DataFrame([total_row])], ignore_index=True)

sentiment_df

location_sentiment = df.groupby('location')[aspects].apply(lambda x: (x == 'positive').sum()).reset_index()

# Melt the DataFrame for easier plotting
location_sentiment_melted = location_sentiment.melt(id_vars='location', var_name='Aspect', value_name='Positive Count')

# Plotting sentiment count per location and aspect
plt.figure(figsize=(15, 8))
sns.barplot(x='location', y='Positive Count', hue='Aspect', data=location_sentiment_melted)
plt.title('Positive Sentiment Count per Location and Aspect')
plt.xlabel('Location')
plt.ylabel('Number of Positive Sentiments')
plt.xticks(rotation=90)
plt.legend(title='Aspect')
plt.tight_layout()
plt.show()


location_sentiment_negative = df.groupby('location')[aspects].apply(lambda x: (x == 'negative').sum()).reset_index()

# Melt the DataFrame for easier plotting
location_sentiment_negative_melted = location_sentiment_negative.melt(id_vars='location', var_name='Aspect', value_name='Negative Count')

# Plotting sentiment count per location and aspect
plt.figure(figsize=(15, 8))
sns.barplot(x='location', y='Negative Count', hue='Aspect', data=location_sentiment_negative_melted)
plt.title('Negative Sentiment Count per Location and Aspect')
plt.xlabel('Location')
plt.ylabel('Number of Negative Sentiments')
plt.xticks(rotation=90)
plt.legend(title='Aspect')
plt.tight_layout()
plt.show()

"""## Stars/rating"""

df['stars'].value_counts().sort_index().plot(kind='bar')

# prompt: stars per location, make it plot bar

import matplotlib.pyplot as plt
stars_per_location = df.groupby('location')['stars'].value_counts().unstack().fillna(0)
stars_per_location.plot(kind='bar', figsize=(10, 6))
plt.title('Stars per Location')
plt.xlabel('Location')
plt.ylabel('Number of Stars')
plt.xticks(rotation=45)
plt.legend(title='Stars')
plt.tight_layout()
plt.show()

"""##Tren waktu"""

# Format kolom tanggal
df['publishedAtDate'] = pd.to_datetime(df['publishedAtDate'], dayfirst=True)

# Tren jumlah ulasan dari waktu ke waktu
monthly_trend = df['publishedAtDate'].dt.to_period('M').value_counts().sort_index()
plt.figure(figsize=(10,4))
monthly_trend.plot()
plt.title('Tren Jumlah Ulasan per Bulan')
plt.xlabel('Bulan')
plt.ylabel('Jumlah Ulasan')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""##Wordcloud"""

# prompt: wordcloud for df

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# Combine all reviews into a single string
text = " ".join(review for review in df['textTranslated'])

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400,
                      background_color='white',
                      stopwords=STOPWORDS,
                      min_font_size=10).generate(text)

# Display the generated image:
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'attraction'

fig, axs = plt.subplots(3,1, figsize=(15, 27))

for i, sentiment in enumerate(['positive', 'negative', 'none']):
    texts = df[df[aspek] == sentiment]['textTranslated'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'amenities'

fig, axs = plt.subplots(3,1, figsize=(15, 27))

for i, sentiment in enumerate(['positive', 'negative', 'none']):
    texts = df[df[aspek] == sentiment]['textTranslated'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'access'

fig, axs = plt.subplots(3,1, figsize=(15, 27))

for i, sentiment in enumerate(['positive', 'negative', 'none']):
    texts = df[df[aspek] == sentiment]['textTranslated'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'price'

fig, axs = plt.subplots(3,1, figsize=(15, 27))

for i, sentiment in enumerate(['positive', 'negative', 'none']):
    texts = df[df[aspek] == sentiment]['textTranslated'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

"""##Panjang teks"""

import matplotlib.pyplot as plt
# Panjang ulasan
df['text_length'] = df['cleaned_text'].astype(str).apply(lambda x: len(x.split()))
plt.figure(figsize=(12,8))
df['text_length'].hist(bins=50, color='blue')
plt.xlim(left=0)
plt.ylim(bottom=0)
plt.title('Distribusi Panjang Ulasan (dalam kata)')
plt.xlabel('Jumlah Kata')
plt.ylabel('Jumlah Ulasan')
plt.show()

"""#PRE-PROCESSING




"""

from sklearn.model_selection import train_test_split

# Fitur dan label
X_input = df['textTranslated']  # atau X_stemmed
y_input = df[['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from tqdm import tqdm
import numpy as np
import random
import copy
import collections
import math
import matplotlib.pyplot as plt

# Fungsi seed
def seed_everything(seed=42):
    random.seed(seed)
    np.random.seed(seed)

# Class MultilabelOversampler (sama seperti sebelumnya)
class MultilabelOversampler:
    def __init__(self, number_of_adds=1000, number_of_tries=100, tqdm_disable=False, details=False, plot=False):
        self.number_of_adds = number_of_adds or 1e6
        self.number_of_tries = number_of_tries or 1e6
        self.tqdm_disable = tqdm_disable
        self.details = details
        self.plot = plot

    def fit(self, df, target_list):
        self.reset()
        self.target_list = target_list
        self.df = copy.deepcopy(df)
        df_new = copy.deepcopy(df)
        res_std, res_bad = [], []

        print("Start the upsampling process.")
        for iter_ in tqdm(range(self.number_of_adds), desc="Iteration", disable=self.tqdm_disable):
            current_std = df_new[self.target_list].sum().std()
            not_working = []
            for try_ in range(self.number_of_tries):
                random_row = self.df.sample(n=1)
                df_interim = pd.concat((df_new, random_row))
                new_std = df_interim[self.target_list].sum().std()
                if new_std < current_std:
                    df_new = df_interim
                    res_std.append(new_std)
                    break
                else:
                    not_working.append((random_row.index[0], new_std))
            if (try_ + 1) == self.number_of_tries:
                print(f"Iter {iter_}: No improvement after {self.number_of_tries} tries.")
                break
            res_bad.append(not_working)

        self.df_new = df_new
        self.res_std = res_std
        self.res_bad = res_bad
        return df_new

    def reset(self):
        self.df = None
        self.df_new = None
        self.res_std = None
        self.res_bad = None

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.utils import resample

# --- 3. Gabungkan kembali train untuk oversampling per label ---
df_train = pd.concat([
    X_train.reset_index(drop=True),
    pd.DataFrame(y_train, columns=['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price'])
], axis=1)

# --- 4. Fungsi oversampling per label (lebih sederhana dari std-based) ---
def multilabel_oversample(df, label_cols):
    df_balanced = df.copy()
    for col in label_cols:
        count_1 = (df_balanced[col] == 1).sum()
        count_0 = (df_balanced[col] == 0).sum()

        if count_1 > count_0:
            minority_class = 0
            diff = count_1 - count_0
        elif count_0 > count_1:
            minority_class = 1
            diff = count_0 - count_1
        else:
            continue

        df_minority = df_balanced[df_balanced[col] == minority_class]
        df_upsampled = resample(df_minority, replace=True, n_samples=diff, random_state=42)

        df_balanced = pd.concat([df_balanced, df_upsampled], ignore_index=True)

    return df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# --- 5. Jalankan oversampling ---
label_cols = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']
df_train_balanced = multilabel_oversample(df_train, label_cols)

# --- 6. Pisahkan kembali fitur dan label ---
X_train_balanced = df_train_balanced['textTranslated'].values
y_train_balanced = df_train_balanced[label_cols].values

# --- 7. (Opsional) Cek distribusi ---
for col in label_cols:
    print(f"\n{col}:\n{df_train_balanced[col].value_counts()}")



# prompt: count value in all table aspect_

# Assuming 'df' is your DataFrame as defined in the provided code.
# Access the counts of each value in the 'aspect_*' columns

aspect_columns = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

for column in aspect_columns:
  value_counts = train_df[column].value_counts()
  print(f"Value counts for {column}:\n{value_counts}\n")

test_df = pd.DataFrame(X_test).copy()
test_df[['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']] = y_test

# Gabungkan kembali X_train dan y_train ke dalam satu DataFrame
train_df = pd.DataFrame(X_train).copy()
train_df.columns = ['textTranslated']
train_df[['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']] = y_train

# Inisialisasi dan jalankan oversampler
seed_everything(42)
mlo = MultilabelOversampler(number_of_adds=10000, number_of_tries=10000, plot=True)

# Jalankan fit hanya ke data train
train_df_oversampled = mlo.fit(train_df, target_list=[
    'aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price'
])

# Setelah oversampling, pisahkan lagi jadi X dan y
X_train_over = train_df_oversampled['textTranslated'].values
y_train_over = train_df_oversampled[[
    'aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price'
]].values

# 10000
aspect_columns = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

for column in aspect_columns:
  value_counts = train_df_oversampled[column].value_counts()
  print(f"Value counts for {column}:\n{value_counts}\n")

# 3500
aspect_columns = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

for column in aspect_columns:
  value_counts = train_df_oversampled[column].value_counts()
  print(f"Value counts for {column}:\n{value_counts}\n")

mlo.plot_all_tries()

# 2000
aspect_columns = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

for column in aspect_columns:
  value_counts = train_df_oversampled[column].value_counts()
  print(f"Value counts for {column}:\n{value_counts}\n")

train_df_oversampled.count()

"""## case folding"""

# Case Folding
df['textTranslated'] = df['textTranslated'].str.lower()
df

"""##Data Cleaning"""

from indoNLP.preprocessing import remove_html, remove_url, replace_word_elongation, emoji_to_words
import re

def clean_text(text):
    text = remove_html(text)
    text = remove_url(text)
    text = replace_word_elongation(text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['textTranslated'] = df['textTranslated'].apply(clean_text)

df.drop_duplicates(inplace=True)
df.reset_index(drop=True, inplace=True)

df

"""## Normalisasi"""

from Singkatan.SingkatanConverter import SingkatanConverter
from indoNLP.preprocessing import replace_slang

normalization_dict = {
    'ngga': 'tidak',
    'nggak': 'tidak',
    'gak': 'tidak',
    'ga': 'tidak',
    'g' : 'tidak',
    'enggak': 'tidak',
    'ngak': 'tidak',
    'kagak': 'tidak',
    'tdk': 'tidak',
    'gaada': 'tidak ada',
    'gakada': 'tidak ada',
}

sc = SingkatanConverter()

# Normalization
def normalize_text(text):
  text = sc.convert(text)
  text = replace_slang(text)
  tokens = text.split()
  tokens = [normalization_dict.get(token, token) for token in tokens]
  return ' '.join(tokens)

dfx = df
dfx['textTranslated'] = df['textTranslated'].apply(normalize_text)
dfx

"""## Stopwords Removal"""

stopwords = [
    'saya', 'aku', 'kamu', 'dia', 'kami', 'kita', 'mereka', 'nya', 'dan', 'atau', 'serta', 'dengan', 'tanpa', 'agar', 'supaya',
    'karena', 'sehingga', 'bila', 'jika', 'walaupun', 'meskipun', 'padahal', 'di', 'ke', 'dari', 'pada', 'untuk', 'dalam', 'oleh',
    'terhadap', 'sudah', 'belum', 'akan', 'sedang', 'masih', 'telah', 'telat', 'yang', 'itu', 'ini', 'adalah', 'yaitu', 'merupakan',
    'sebagai', 'tersebut', 'pun', 'saja', 'lah', 'deh', 'dong', 'nih', 'loh', 'ya', 'kok', 'kan', 'toh', 'lalu', 'bagi', 'hal',
    'ketika', 'saat', 'bahwa', 'secara', 'lain', 'anda', 'begitu', 'mengapa', 'kenapa', 'yakni', 'itulah', 'lagi', 'maka', 'demi',
    'dimana', 'kemana', 'pula', 'sambil', 'supaya', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi', 'sementara', 'apakah', 'kecuali',
    'selain', 'seolah', 'seraya', 'seterusnya', 'boleh', 'dapat', 'dahulu', 'dulunya', 'anu', 'demikian', 'mari', 'nanti',
    'melainkan', 'oh', 'seharusnya', 'sebetulnya', 'setiap', 'setidaknya', 'duga', 'sana', 'sini', 'pernah', 'datang', 'sisi', 'juga'
]


def remove_stopwords(text, stopwords):
    tokens = text.lower().split()
    filtered = [word for word in tokens if word not in stopwords]
    return ' '.join(filtered)

dfx['textTranslated'] = dfx['textTranslated'].apply(lambda x: remove_stopwords(x, stopwords))

dfx

"""##Stemming"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

df['stemmed_text'] = df['textTranslated'].dropna().astype(str).apply(lambda x: stemmer.stem(x))


df[['textTranslated', 'stemmed_text']]

df = df.rename(columns={'textTranslated': 'cleaned_text'})
df

df.to_csv('data_clean.csv', index=False)

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'attraction'

fig, axs = plt.subplots(3,1, figsize=(15, 27))

for i, sentiment in enumerate(['positive', 'negative', 'none']):
    texts = df[df[aspek] == sentiment]['stemmed_text'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'amenities'

fig, axs = plt.subplots(3,1, figsize=(15, 27))

for i, sentiment in enumerate(['positive', 'negative', 'none']):
    texts = df[df[aspek] == sentiment]['stemmed_text'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'access'

fig, axs = plt.subplots(3,1, figsize=(15, 27))

for i, sentiment in enumerate(['positive', 'negative', 'none']):
    texts = df[df[aspek] == sentiment]['stemmed_text'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'price'

fig, axs = plt.subplots(3,1, figsize=(15, 27))

for i, sentiment in enumerate(['positive', 'negative', 'none']):
    texts = df[df[aspek] == sentiment]['stemmed_text'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

"""##Encoding"""

# Aspect labels
df['aspect_attraction'] = df['attraction'].apply(lambda x: 0 if x == 'none' else 1)
df['aspect_amenities'] = df['amenities'].apply(lambda x: 0 if x == 'none' else 1)
df['aspect_access'] = df['access'].apply(lambda x: 0 if x == 'none' else 1)
df['aspect_price'] = df['price'].apply(lambda x: 0 if x == 'none' else 1)
df.head()

df[['cleaned_text', 'aspect_attraction','aspect_amenities','aspect_access','aspect_price'] ]

# prompt: count aspect_attraction 1 and 0

print(df['aspect_attraction'].value_counts())
print(df['aspect_amenities'].value_counts())
print(df['aspect_access'].value_counts())
print(df['aspect_price'].value_counts())

# Sentiment labels
def encode_sentiment(x):
    if x == 'positive':
        return 1
    elif x == 'negative':
        return 0
    else:
        return x

df['sentiment_attraction'] = df['attraction'].apply(encode_sentiment)
df['sentiment_amenities'] = df['amenities'].apply(encode_sentiment)
df['sentiment_access'] = df['access'].apply(encode_sentiment)
df['sentiment_price'] = df['price'].apply(encode_sentiment)
df.head()

"""##WordCloud after preprocess"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'attraction'

fig, axs = plt.subplots(1,2, figsize=(10, 18))

for i, sentiment in enumerate(['positive', 'negative']):
    texts = df[df[aspek] == sentiment]['cleaned_text'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'amenities'

fig, axs = plt.subplots(1,2, figsize=(10, 18))

for i, sentiment in enumerate(['positive', 'negative']):
    texts = df[df[aspek] == sentiment]['cleaned_text'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'access'

fig, axs = plt.subplots(1,2, figsize=(10, 18))

for i, sentiment in enumerate(['positive', 'negative']):
    texts = df[df[aspek] == sentiment]['cleaned_text'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter

aspek = 'price'

fig, axs = plt.subplots(1,2, figsize=(10, 18))

for i, sentiment in enumerate(['positive', 'negative']):
    texts = df[df[aspek] == sentiment]['cleaned_text'].dropna().astype(str).tolist()
    all_text = ' '.join(texts).lower()
    word_freq = Counter(all_text.split())
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].axis('off')
    axs[i].set_title(f'WordCloud - {aspek.capitalize()} ({sentiment})')

plt.tight_layout()
plt.show()

"""#Word Embedding"""

import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import pandas as pd
# Load the CSV file
#file_url = "/content/data_clean.csv"
file_url = "https://drive.google.com/uc?id=1mGwT68gdf7oKj7VMsOVteHMR2_sjwa6O"
df = pd.read_csv(file_url, sep=',')
df.head()

df

df.dropna(subset=['cleaned_text'], inplace=True)
df.dropna(subset=['stemmed_text'], inplace=True)
df.info()

# ===========================================
# 3. TRAIN WORD2VEC (SKIP-GRAM)
# ===========================================
tokenized_reviews = [text.lower().split() for text in df['stemmed_text']]
w2v_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, sg=1)

# ===========================================
# 4. TOKENIZER AND EMBEDDING MATRIX
# ===========================================
vocab_size = 10000
embedding_dim = 100
max_length = 100

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(df['stemmed_text'])
sequences = tokenizer.texts_to_sequences(df['stemmed_text'])
X = pad_sequences(sequences, maxlen=max_length, padding='post')

# Embedding Matrix
word_index = tokenizer.word_index
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in word_index.items():
    if i < vocab_size:
        if word in w2v_model.wv:
            embedding_matrix[i] = w2v_model.wv[word]
        else:
            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))

# prompt: simpan model w2v dengan format .wv dan embedding matrix nya

import numpy as np

# Save the Word2Vec model in .wv format
w2v_model.wv.save("word2vec_model.wv")

# Save the embedding matrix
np.save("embedding_matrix.npy", embedding_matrix)

print("Word2Vec model saved as word2vec_model.wv")
print("Embedding matrix saved as embedding_matrix.npy")

# ===========================
# 1. Word2Vec untuk Original
# ===========================
vocab_size = 10000
embedding_dim = 100
max_length = 100

tokenized_original = [text.lower().split() for text in df['cleaned_text'].dropna()]
w2v_original = Word2Vec(sentences=tokenized_original, vector_size=embedding_dim, window=5, min_count=1, sg=1)

tokenizer_original = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer_original.fit_on_texts(df['cleaned_text'].astype(str))
seq_original = tokenizer_original.texts_to_sequences(df['cleaned_text'].astype(str))
X_original = pad_sequences(seq_original, maxlen=max_length, padding='post')

word_index_ori = tokenizer_original.word_index
embedding_matrix_ori = np.zeros((vocab_size, embedding_dim))
for word, i in word_index_ori.items():
    if i < vocab_size:
        if word in w2v_original.wv:
            embedding_matrix_ori[i] = w2v_original.wv[word]
        else:
            embedding_matrix_ori[i] = np.random.normal(scale=0.6, size=(embedding_dim,))

# ============================
# 2. Word2Vec untuk Stemmed
# ============================
tokenized_stemmed = [text.lower().split() for text in df['stemmed_text'].dropna()]
w2v_stemmed = Word2Vec(sentences=tokenized_stemmed, vector_size=embedding_dim, window=5, min_count=1, sg=1)

tokenizer_stemmed = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer_stemmed.fit_on_texts(df['stemmed_text'].astype(str))
seq_stemmed = tokenizer_stemmed.texts_to_sequences(df['stemmed_text'].astype(str))
X_stemmed = pad_sequences(seq_stemmed, maxlen=max_length, padding='post')

word_index_stem = tokenizer_stemmed.word_index
embedding_matrix_stem = np.zeros((vocab_size, embedding_dim))
for word, i in word_index_stem.items():
    if i < vocab_size:
        if word in w2v_stemmed.wv:
            embedding_matrix_stem[i] = w2v_stemmed.wv[word]
        else:
            embedding_matrix_stem[i] = np.random.normal(scale=0.6, size=(embedding_dim,))

######

# --- Fungsi embed Word2Vec ---
def embed_word2vec(texts, w2v_model, embedding_dim=300):
    embeddings = []
    for doc in texts:
        vectors = [w2v_model[word] for word in doc.split() if word in w2v_model]
        if vectors:
            doc_vec = np.mean(vectors, axis=0)
        else:
            doc_vec = np.zeros(embedding_dim)
        embeddings.append(doc_vec)
    return np.array(embeddings)

# --- Fungsi embed BERT ---
def embed_bert(texts, tokenizer, model, max_len=128):
    # import torch # No longer needed if not using torch tensors
    input_ids = []
    attention_masks = []
    for doc in texts:
        encoded = tokenizer.encode_plus(
            doc,
            add_special_tokens=True,
            max_length=max_len,
            truncation=True,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='tf'  # Change 'pt' to 'tf' for TensorFlow tensors
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    # Concatenate TensorFlow tensors using tf.concat
    input_ids = tf.concat(input_ids, axis=0)
    attention_masks = tf.concat(attention_masks, axis=0)


    # with torch.no_grad(): # No longer needed for TensorFlow model
    outputs = model(input_ids, attention_mask=attention_masks)
    # For TFBertModel, the pooled output is usually the first element of the tuple output
    pooled_output = outputs[0].numpy() # Access the pooled output and convert to numpy
    return pooled_output

def embed_distilbert(texts, tokenizer, model, max_len=128, batch_size=32):
    """
    Embeds text data using DistilBERT in batches to manage memory.
    """
    all_embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        input_ids = []
        attention_masks = []

        for doc in batch_texts:
            encoded = tokenizer.encode_plus(
                doc,
                add_special_tokens=True,
                max_length=max_len,
                truncation=True,
                padding='max_length',
                return_attention_mask=True,
                return_tensors='tf'
            )
            input_ids.append(encoded['input_ids'])
            attention_masks.append(encoded['attention_mask'])

        input_ids = tf.concat(input_ids, axis=0)
        attention_masks = tf.concat(attention_masks, axis=0)

        outputs = model(input_ids, attention_mask=attention_masks)
        pooled_output = tf.reduce_mean(outputs.last_hidden_state, axis=1)
        all_embeddings.append(pooled_output.numpy())

    # Concatenate embeddings from all batches
    return np.concatenate(all_embeddings, axis=0)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report
import numpy as np
from collections import Counter
from sklearn.model_selection import KFold
from sklearn.metrics import precision_score, recall_score, f1_score
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from gensim.models import KeyedVectors

from sklearn.model_selection import train_test_split

# Fitur dan label
X_input = df['cleaned_text']
y_input = df[['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

"""#oversampling

##ML_ROS RandomOverSampler
"""

def multilabel_ros(X, y):
    """
    Multi-Label Random Oversampling (ml_ros)
    Menyeimbangkan jumlah sampel tiap kombinasi label dengan menduplikasi sampel

    Parameters:
    X: np.array fitur, shape (n_samples, n_features)
    y: np.array multilabel biner, shape (n_samples, n_classes)

    Returns:
    X_res, y_res: np.array oversampled data
    """
    df_features = pd.DataFrame(X)
    df_labels = pd.DataFrame(y)
    df = pd.concat([df_features, df_labels], axis=1)

    label_tuples = [tuple(row) for row in y]
    freq = Counter(label_tuples)
    max_count = max(freq.values())

    dfs = []
    for label_combo, count in freq.items():
        df_subset = df[df_labels.apply(tuple, axis=1) == label_combo]
        n_repeat = (max_count // count) - 1
        n_rest = max_count % count

        df_oversampled = pd.concat([df_subset] * n_repeat, ignore_index=True)
        df_oversampled = pd.concat([df_oversampled, df_subset.sample(n=n_rest, replace=True)], ignore_index=True)

        dfs.append(pd.concat([df_subset, df_oversampled], ignore_index=True))

    df_balanced = pd.concat(dfs, ignore_index=True)
    X_res = df_balanced.iloc[:, :X.shape[1]].values
    y_res = df_balanced.iloc[:, X.shape[1]:].values

    return X_res, y_res

def build_cnn_model(input_shape, num_classes=4):
    inputs = tf.keras.Input(shape=input_shape)
    x = tf.keras.layers.Reshape((input_shape[0], 1))(inputs)
    x = tf.keras.layers.Conv1D(128, kernel_size=5, activation='relu')(x)
    x = tf.keras.layers.GlobalMaxPooling1D()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    outputs = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
    )
    return model

from sklearn.model_selection import KFold
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def run_single_skenario(df, labels, w2v_model, bert_tokenizer, bert_model,
                       use_word2vec=True, use_stemming=True, use_oversampling=False,
                       num_classes=4, epochs=10, batch_size=32):
    """
    Pipeline untuk satu skenario:
    - use_word2vec: True untuk Word2Vec, False untuk BERT
    - use_stemming: True pakai kolom 'stemmed_text', False pakai 'cleaned_text'
    - use_oversampling: True pakai ml_ros, False tidak

    Return:
        dict hasil evaluasi: precision, recall, f1, accuracy, classification report, confusion matrix
    """

    kfold = KFold(n_splits=5, shuffle=True, random_state=42)

    precisions, recalls, f1s, accuracies = [], [], [], []
    all_true_labels, all_pred_labels = [], []  # Untuk confusion matrix dan classification report

    for train_idx, test_idx in kfold.split(df):
        df_train, df_test = df.iloc[train_idx], df.iloc[test_idx]
        y_train, y_test = labels[train_idx], labels[test_idx]

        # Pilih teks
        if use_stemming:
            texts_train = df_train['stemmed_text'].astype(str).tolist()
            texts_test = df_test['stemmed_text'].astype(str).tolist()
        else:
            texts_train = df_train['cleaned_text'].astype(str).tolist()
            texts_test = df_test['cleaned_text'].astype(str).tolist()

        # Embedding
        if use_word2vec:
            X_train = embed_word2vec(texts_train, w2v_model)
            X_test = embed_word2vec(texts_test, w2v_model)
        else:
            X_train = embed_distilbert(texts_train, bert_tokenizer, bert_model)
            X_test = embed_distilbert(texts_test, bert_tokenizer, bert_model)

        # Oversampling jika aktif
        if use_oversampling:
            X_train, y_train = multilabel_ros(X_train, y_train)

        # Build dan train CNN
        cnn_model = build_cnn_model(input_shape=X_train.shape[1:], num_classes=num_classes)
        cnn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)

        # Prediksi & evaluasi
        y_pred_prob = cnn_model.predict(X_test)
        y_pred = (y_pred_prob > 0.5).astype(int)

        # Akurasi
        accuracy = accuracy_score(y_test, y_pred)
        accuracies.append(accuracy)

        # Precision, Recall, F1 Score
        precisions.append(precision_score(y_test, y_pred, average='macro', zero_division=0))
        recalls.append(recall_score(y_test, y_pred, average='macro', zero_division=0))
        f1s.append(f1_score(y_test, y_pred, average='macro', zero_division=0))

        # Tambahkan untuk confusion matrix dan classification report
        all_true_labels.append(y_test)
        all_pred_labels.append(y_pred)

    # Rata-rata metrik dari semua fold
    avg_precision = np.mean(precisions)
    avg_recall = np.mean(recalls)
    avg_f1 = np.mean(f1s)
    avg_accuracy = np.mean(accuracies)

    # Gabungkan hasil label asli dan prediksi untuk confusion matrix dan classification report
    all_true_labels = np.vstack(all_true_labels)
    all_pred_labels = np.vstack(all_pred_labels)

    # Classification Report dan Confusion Matrix
    print("Classification Report:")
    target_names = ['attraction', 'amenities', 'access', 'price']
    print(classification_report(all_true_labels, all_pred_labels, target_names=target_names))

    cm = confusion_matrix(all_true_labels.argmax(axis=1), all_pred_labels.argmax(axis=1))  # Jika multilabel
    print("Confusion Matrix:")
    print(cm)

    # Plot Confusion Matrix
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=target_names,
                yticklabels=target_names)
    plt.ylabel('True Labels')
    plt.xlabel('Predicted Labels')
    plt.title('Confusion Matrix')
    plt.show()

    return {
        'precision': avg_precision,
        'recall': avg_recall,
        'f1_score': avg_f1,
        'accuracy': avg_accuracy
    }

from gensim.models import Word2Vec

sentences = [text.split() for text in X_train['cleaned_text']]

model_skipgram = Word2Vec(
    sentences,
    vector_size=300,    # dimensi embedding
    window=5,           # ukuran konteks
    min_count=2,        # abaikan kata yang muncul kurang dari 2 kali
    sg=1,               # 1 untuk skip-gram, 0 untuk CBOW
    workers=4,          # jumlah thread
    epochs=10           # jumlah iterasi pelatihan
)

model_skipgram.save("word2vec_ori.model")

from gensim.models import Word2Vec

sentences = [text.split() for text in X_train['stemmed_text']]

model_skipgram = Word2Vec(
    sentences,
    vector_size=300,    # dimensi embedding
    window=5,           # ukuran konteks
    min_count=2,        # abaikan kata yang muncul kurang dari 2 kali
    sg=1,               # 1 untuk skip-gram, 0 untuk CBOW
    workers=4,          # jumlah thread
    epochs=10           # jumlah iterasi pelatihan
)

model_skipgram.save("word2vec_stem.model")

# Load BERT tokenizer dan model
from transformers import BertTokenizer, TFBertModel
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')

!pip install transformers

from transformers import DistilBertTokenizer, TFDistilBertModel

bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')
bert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased')

# Load model Word2Vec
from gensim.models import Word2Vec
word2vec_model = Word2Vec.load('/content/word2vec_ori.model')

result = run_single_skenario(
    df=X_train,
    labels=y_train,
    w2v_model=word2vec_model.wv,
    bert_tokenizer=bert_tokenizer,
    bert_model=bert_model,
    use_word2vec=True,
    use_stemming=False,
    use_oversampling=False
)
print(result)

# Load model Word2Vec
from gensim.models import Word2Vec
word2vec_model = Word2Vec.load('/content/word2vec_stem.model')

result = run_single_skenario(
    df=X_train,
    labels=y_train,
    w2v_model=word2vec_model.wv,
    bert_tokenizer=bert_tokenizer,
    bert_model=bert_model,
    use_word2vec=True,
    use_stemming=True,
    use_oversampling=False
)
print(result)

# Load model Word2Vec
from gensim.models import Word2Vec
word2vec_model = Word2Vec.load('/content/word2vec_ori.model')

result = run_single_skenario(
    df=X_train,
    labels=y_train,
    w2v_model=word2vec_model.wv,
    bert_tokenizer=bert_tokenizer,
    bert_model=bert_model,
    use_word2vec=False,
    use_stemming=False,
    use_oversampling=False
)

result

# Load model Word2Vec
from gensim.models import Word2Vec
word2vec_model = Word2Vec.load('/content/word2vec_stem.model')

result = run_single_skenario(
    df=X_train,
    labels=y_train,
    w2v_model=word2vec_model.wv,
    bert_tokenizer=bert_tokenizer,
    bert_model=bert_model,
    use_word2vec=False,
    use_stemming=False,
    use_oversampling=False
)

result

"""##ML_ROS"""

import pandas as pd
from sklearn.utils import resample
from sklearn.model_selection import train_test_split

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

# Ubah X dan y menjadi DataFrame
X_train_df = pd.DataFrame(X_train)
y_train_df = pd.DataFrame(y_train, columns=['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price'])

# Gabungkan X dan y
df_train = pd.concat([X_train_df.reset_index(drop=True), y_train_df.reset_index(drop=True)], axis=1)

# Kolom label multilabel
aspect_columns = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

# Salin data awal untuk oversampling
df_balanced = df_train.copy()

# Oversampling per label
for aspect in aspect_columns:
    df_1 = df_train[df_train[aspect] == 1]
    df_0 = df_train[df_train[aspect] == 0]

    # Selisih jumlah sampel
    diff = abs(len(df_1) - len(df_0))

    if len(df_1) < len(df_0):
        df_minority_upsampled = resample(df_1, replace=True, n_samples=diff, random_state=42)
    elif len(df_1) > len(df_0):
        df_minority_upsampled = resample(df_0, replace=True, n_samples=diff, random_state=42)
    else:
        df_minority_upsampled = pd.DataFrame()  # Sudah seimbang

    # Gabungkan hasil oversampling
    df_balanced = pd.concat([df_balanced, df_minority_upsampled], axis=0)

# Shuffle hasil akhir
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Pisahkan kembali ke X dan y
X_train_os = df_balanced.iloc[:, 0].values  # text (kolom pertama)
y_train_os = df_balanced[aspect_columns].values  # multilabel target

# (Opsional) Simpan ke file
# df_balanced.to_csv("train_oversampled.csv", index=False)

print(df_balanced['aspect_attraction'].value_counts())
print(df_balanced['aspect_amenities'].value_counts())
print(df_balanced['aspect_access'].value_counts())
print(df_balanced['aspect_price'].value_counts())

def create_multilabel_cnn(vocab_size, embedding_dim, max_length, embedding_matrix):
    model = Sequential()
    model.add(Embedding(input_dim=vocab_size,
                        output_dim=embedding_dim,
                        weights=[embedding_matrix],
                        trainable=False))
    model.add(Conv1D(128, 5, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4, activation='sigmoid'))  # Output 4 aspek
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

from sklearn.metrics import classification_report
from sklearn.model_selection import StratifiedKFold
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout

# Assuming create_multilabel_cnn, vocab_size, embedding_dim, max_length,
# and w2v_original are defined in previous cells as in the user's notebook.

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
fold = 1

# Agar stratifikasi bisa jalan, ubah label menjadi 1D (gabungkan semua label ke string)
# Ensure y_train is a numpy array before converting to list of strings
y_stratify = y_train.astype(str).tolist()
y_stratify = [''.join(row) for row in y_stratify]

# Convert X_train from Series to numpy array before splitting with KFold
# This ensures that integer indices from kf.split work correctly
X_train_np = X_train.values

for train_idx, val_idx in kf.split(X_train_np, y_stratify):
    print(f"\n===== Fold {fold} =====")

    # Use positional indexing (.iloc) for both the numpy array and the y_train numpy array
    X_tr, X_val = X_train_np[train_idx], X_train_np[val_idx]
    y_tr, y_val = y_train[train_idx], y_train[val_idx]

    # Ensure that X_tr and X_val are in the correct format for the tokenizer
    # Convert elements to string, handling potential non-string types (like NaN which is float)
    X_tr = [str(item) for item in X_tr]
    X_val = [str(item) for item in X_val]


    # Tokenize and pad sequences for each fold
    # You need to redefine tokenizer and sequences/padding within the loop
    # to fit the data for the current fold
    fold_tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
    fold_tokenizer.fit_on_texts(X_tr)

    X_tr_seq = fold_tokenizer.texts_to_sequences(X_tr)
    X_tr_padded = pad_sequences(X_tr_seq, maxlen=max_length, padding='post')

    X_val_seq = fold_tokenizer.texts_to_sequences(X_val)
    X_val_padded = pad_sequences(X_val_seq, maxlen=max_length, padding='post')

    # Rebuild embedding matrix for the current fold's tokenizer if using Word2Vec
    # This requires having the Word2Vec model available
    # Assuming w2v_original is your trained Word2Vec model
    fold_word_index = fold_tokenizer.word_index
    fold_embedding_matrix = np.zeros((vocab_size, embedding_dim))
    for word, i in fold_word_index.items():
        if i < vocab_size:
            # Check if word is in w2v_original.wv
            if word in w2v_original.wv:
                fold_embedding_matrix[i] = w2v_original.wv[word]
            else:
                # Handle words not in w2v_original (can use random or zeros)
                fold_embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))


    # Pass the fold-specific embedding matrix to the model
    model = create_multilabel_cnn(vocab_size, embedding_dim, max_length, fold_embedding_matrix)
    model.fit(X_tr_padded, y_tr, epochs=10, batch_size=32, verbose=1)

    y_pred = (model.predict(X_val_padded) > 0.5).astype(int)

    print(classification_report(y_val, y_pred, target_names=['attraction', 'amenities', 'access', 'price']))
    fold += 1

print("\n===== Evaluasi Akhir pada Test Set =====")
y_pred_test = (model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred_test, target_names=['attraction', 'amenities', 'access', 'price']))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

labels = ['attraction', 'amenities', 'access', 'price']
for i, label in enumerate(labels):
    cm = confusion_matrix(y_test[:, i], y_pred_test[:, i])
    plt.figure(figsize=(3,2))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {label}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

!pip install keras-tuner

import keras_tuner as kt
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam

def build_model(hp):
    model = models.Sequential()
    model.add(Embedding(input_dim=vocab_size,
                        output_dim=embedding_dim,
                        weights=[embedding_matrix_stem],
                        trainable=False))
    model.add(layers.Conv1D(filters=hp.Int('filters', 32, 256, step=32),
                            kernel_size=hp.Choice('kernel_size', [3, 4, 5]),
                            activation='relu'))
    model.add(layers.GlobalMaxPooling1D())
    model.add(layers.Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))
    model.add(layers.Dense(hp.Int('dense_units', 64, 256, step=64), activation='relu'))
    model.add(layers.Dense(4, activation='sigmoid'))  # Output untuk multi-label

    model.compile(optimizer=Adam(
                      learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='log')),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])  # Accuracy bisa diganti dengan f1 nanti
    return model

tuner = kt.BayesianOptimization(
    build_model,
    objective='val_accuracy',  # Bisa diganti kalau pakai custom F1
    max_trials=10,
    directory='bayes_opt',
    project_name='cnn_aspect_tuning3')


tuner.search(X_train, y_train,
             validation_split=0.1,
             epochs=10,
             batch_size=32)

best_model = tuner.get_best_models(num_models=1)[0]

# Evaluasi di test set yang 10%
best_model.evaluate(X_test, y_test)

# Jika ingin pakai f1_score
from sklearn.metrics import classification_report

y_pred = (best_model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred, target_names=['attraction', 'amenities', 'access', 'price']))

!pip install scikit-learn==1.3.0

from sklearn.metrics import f1_score
from sklearn.model_selection import KFold
from tensorflow.keras import backend as K
import keras_tuner as kt
import numpy as np
from imblearn.over_sampling import RandomOverSampler
from sklearn.multiclass import OneVsRestClassifier # Import OneVsRestClassifier


# Wrap RandomOverSampler in OneVsRestClassifier for Multi-Label
ros = RandomOverSampler(random_state=42)
multilabel_oversampler = OneVsRestClassifier(ros)

# Fit and Resample your data
X, y = multilabel_oversampler.fit_resample(X_train, y_train)


#X, y = RandomOverSampler().fit_resample(X_train, y_train)

class MyHyperModel(kt.HyperModel):
    def build(self, hp):
        return build_model(hp)


    def fit(self, hp, model, x, y, **kwargs):
        kf = KFold(n_splits=5)
        f1_scores = []

        for train_idx, val_idx in kf.split(x):
            x_train, x_val = x[train_idx], x[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            model = self.build(hp)  # build ulang model tiap fold
            model.fit(x_train, y_train,
                      epochs=kwargs.get('epochs', 10),
                      batch_size=kwargs.get('batch_size', 32),
                      verbose=0)

            preds = model.predict(x_val)
            preds_binary = (preds > 0.5).astype(int)
            f1 = f1_score(y_val, preds_binary, average='micro')  # atau 'macro'
            f1_scores.append(f1)

            K.clear_session()  # mencegah memory leak

        mean_f1 = np.mean(f1_scores)
        return {'val_f1': mean_f1}

# Tuner setup
tuner = kt.BayesianOptimization(
    MyHyperModel(),
    objective=kt.Objective('val_f1', direction='max'),
    max_trials=10,
    directory='bayes_f1_opt',
    project_name='cnn_multilabel_f1'
)

# Jalankan pencarian
tuner.search(X, y, epochs=10, batch_size=32)

best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]

# Bangun model terbaik berdasarkan hyperparameter optimal
best_model = build_model(best_hp)

best_model.fit(X_train, y_train,
               epochs=10,  # atau bisa lebih
               batch_size=32,
               verbose=1)

y_pred = (best_model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred, target_names=['attraction', 'amenities', 'access', 'price']))

best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]

# Cetak nilai-nilai optimalnya
print("Best Parameters Found:")
print("Filters:", best_hp.get('filters'))
print("Kernel Size:", best_hp.get('kernel_size'))
print("Dropout Rate:", best_hp.get('dropout_rate'))
print("Dense Units:", best_hp.get('dense_units'))
print("Learning Rate:", best_hp.get('lr'))

"""#Modelling"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from imblearn.over_sampling import RandomOverSampler
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from transformers import BertTokenizer, TFBertModel
import tensorflow as tf
import re, string

"""##split"""

# Fitur dan label

df.dropna(inplace=True)
X_input = df[['cleaned_text', 'stemmed_text']]
y_input = df[['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

# prompt: hapus baris yang mengandung nilai kosong

df.dropna(inplace=True)
df.isnull().sum()

# Fitur dan label
X_input = df[['cleaned_text', 'stemmed_text']]
y_input = df[['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

df_train = pd.concat([
    X_train.reset_index(drop=True),
    pd.DataFrame(y_train, columns=['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price'])
], axis=1)

aspect_columns = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

for column in aspect_columns:
  # Count the occurrences of 0 and 1 in the current aspect column
  value_counts = df[column].value_counts()

  # Print the counts for the current aspect column
  print(f"Value counts for {column}:")
  if 1 in value_counts:
    print(f"  1: {value_counts[1]}")
  else:
    print(f"  1: 0")
  if 0 in value_counts:
    print(f"  0: {value_counts[0]}")
  else:
    print(f"  0: 0")
  print("-" * 20) # Print a separator for better readability

aspect_columns = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

for column in aspect_columns:
  # Count the occurrences of 0 and 1 in the current aspect column
  value_counts = df_train[column].value_counts()

  # Print the counts for the current aspect column
  print(f"Value counts for {column}:")
  if 1 in value_counts:
    print(f"  1: {value_counts[1]}")
  else:
    print(f"  1: 0")
  if 0 in value_counts:
    print(f"  0: {value_counts[0]}")
  else:
    print(f"  0: 0")
  print("-" * 20) # Print a separator for better readability

df_test = pd.concat([
    X_test.reset_index(drop=True),
    pd.DataFrame(y_test, columns=['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price'])
], axis=1)

aspect_columns = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

for column in aspect_columns:
  # Count the occurrences of 0 and 1 in the current aspect column
  value_counts = df_test[column].value_counts()

  # Print the counts for the current aspect column
  print(f"Value counts for {column}:")
  if 1 in value_counts:
    print(f"  1: {value_counts[1]}")
  else:
    print(f"  1: 0")
  if 0 in value_counts:
    print(f"  0: {value_counts[0]}")
  else:
    print(f"  0: 0")
  print("-" * 20) # Print a separator for better readability

"""##baru"""

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel

def bert_encode(texts, tokenizer, model_bert, max_len=100, batch_size=16):
    outputs_all = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        encodings = tokenizer(
            batch_texts.tolist(),
            truncation=True,
            padding='max_length',
            max_length=max_len,
            return_tensors="tf"
        )
        outputs = model_bert(encodings['input_ids'], attention_mask=encodings['attention_mask'])
        last_hidden = outputs.last_hidden_state  # shape: (batch_size, max_len, hidden_size)
        outputs_all.append(last_hidden.numpy())

    return np.vstack(outputs_all)  # final shape: (total_samples, max_len, hidden_size)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001):
    input_layer = Input(shape=input_shape)  # now input_shape = (100, 768)

    conv_3 = Conv1D(128, 3, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(128, 4, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(128, 5, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(0.3)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0003)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

# --------------------- Word2Vec Tokenization ---------------------
def tokenize_word2vec(texts, tokenizer, max_len=100):
    sequences = [tokenizer(text) for text in texts]
    return pad_sequences(sequences, maxlen=max_len, padding='post')

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel

def bert_encode(texts, tokenizer, model_bert, max_len=100, batch_size=32):
    outputs_all = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        encodings = tokenizer(
            batch_texts.tolist(),
            truncation=True,
            padding='max_length',  # pakai 'max_length' supaya semua batch sama panjang
            max_length=max_len,
            return_tensors="tf"
        )
        outputs = model_bert(encodings['input_ids'], attention_mask=encodings['attention_mask'])
        # Ambil CLS token (index 0)
        last_hidden = outputs.last_hidden_state[:, 0, :]
        outputs_all.append(last_hidden.numpy())

    return np.vstack(outputs_all)

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel

# Fungsi untuk encode BERT dengan fine-tuning embedding
def bert_encode(texts, tokenizer, model_bert, max_len=100, batch_size=32):
    outputs_all = []

    # Freeze all layers except embeddings
    for layer in model_bert.layers:
        layer.trainable = False
    model_bert.distilbert.embeddings.trainable = True  # Unfreeze embedding layer

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        encodings = tokenizer(
            batch_texts.tolist(),
            truncation=True,
            padding='max_length',  # pakai 'max_length' supaya semua batch sama panjang
            max_length=max_len,
            return_tensors="tf"
        )
        # Panggil model BERT untuk mengeluarkan embedding
        outputs = model_bert(encodings['input_ids'], attention_mask=encodings['attention_mask'])
        last_hidden = outputs.last_hidden_state[:, 0, :]  # Mengambil CLS token
        outputs_all.append(last_hidden.numpy())

    return np.vstack(outputs_all)

from sklearn.utils import resample

def oversample(df, text_col, label_cols, max_upsample=1000):
    df_balanced = df.copy()
    for aspect in label_cols:
        df_1 = df[df[aspect] == 1]
        df_0 = df[df[aspect] == 0]

        diff = abs(len(df_1) - len(df_0))

        # Batasi jumlah penambahan maksimal max_upsample
        n_samples = min(diff, max_upsample)

        if len(df_1) < len(df_0) and n_samples > 0:
            df_minority_upsampled = resample(df_1, replace=True, n_samples=n_samples, random_state=42)
        elif len(df_1) > len(df_0) and n_samples > 0:
            df_minority_upsampled = resample(df_0, replace=True, n_samples=n_samples, random_state=42)
        else:
            df_minority_upsampled = pd.DataFrame()

        df_balanced = pd.concat([df_balanced, df_minority_upsampled], axis=0)

    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)
    return df_balanced[[text_col] + label_cols]

def create_cnn_model(vocab_size, embedding_dim, num_classes):
    model = Sequential([
        Embedding(vocab_size, embedding_dim),
        Conv1D(96, 4, activation='relu'),
        GlobalMaxPooling1D(),
        Dropout(0.5),
        Dense(256, activation='relu'),
        Dense(num_classes, activation='sigmoid')
    ])
    opt = Adam(learning_rate=0.0005)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

"""##w2v"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_cnn_model_pretrained(vocab_size, embedding_dim, max_length, num_classes, embedding_matrix, l2_strength=0.001):
    input_layer = Input(shape=(max_length,))
    embedding_layer = Embedding(
        input_dim=vocab_size,
        output_dim=embedding_dim,
        weights=[embedding_matrix],
        input_length=max_length,
        trainable=False
    )(input_layer)

    conv_3 = Conv1D(96, 3, activation='relu', kernel_regularizer=l2(l2_strength))(embedding_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(96, 4, activation='relu', kernel_regularizer=l2(l2_strength))(embedding_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(96, 5, activation='relu', kernel_regularizer=l2(l2_strength))(embedding_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(0.3)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0001)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import KFold

def run_pipeline(data, skenario, num_folds=5):
    texts = data['stemmed_text'] if skenario['stem'] else data['cleaned_text']
    label_cols = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']
    mlb = MultiLabelBinarizer()
    mlb.classes_ = label_cols
    y = data[label_cols].values

    # K-Fold
    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)
    accs, f1s, precisions, recalls = [], [], [], []

    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):
        print(f"\n===== Fold {fold + 1} =====")

        df_train_fold = data.iloc[train_idx].reset_index(drop=True)
        df_val_fold = data.iloc[val_idx].reset_index(drop=True)

        text_col = 'stemmed_text' if skenario['stem'] else 'cleaned_text'

        # Oversampling pada data mentah train fold
        if skenario["oversample"]:
            df_train_fold = oversample(df_train_fold, text_col, label_cols)
            print(df_train_fold['aspect_attraction'].value_counts())
            print(df_train_fold['aspect_amenities'].value_counts())
            print(df_train_fold['aspect_access'].value_counts())
            print(df_train_fold['aspect_price'].value_counts())

        texts_train = df_train_fold[text_col].astype(str)
        y_train = df_train_fold[label_cols].values

        texts_val = df_val_fold[text_col].astype(str)
        y_val = df_val_fold[label_cols].values

        # Embedding per fold
        if skenario["embedding"] == "word2vec":
            # --- Train Word2Vec model on the training fold data ---
            tokenized_train = [text.split() for text in texts_train]
            w2v_model = Word2Vec(tokenized_train, vector_size=100, min_count=1, sg=1)

            # --- Tokenizer and Embedding Matrix per fold ---
            vocab_size = 10000  # Still use a fixed vocab size for consistency with model architecture
            embedding_dim = 100
            max_length = 100

            tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
            tokenizer.fit_on_texts(texts_train) # Fit tokenizer on training fold
            sequences_train = tokenizer.texts_to_sequences(texts_train)
            sequences_val = tokenizer.texts_to_sequences(texts_val)

            X_train = pad_sequences(sequences_train, maxlen=max_length, padding='post')
            X_val = pad_sequences(sequences_val, maxlen=max_length, padding='post')


            # --- Create Embedding Matrix using the fold's tokenizer word_index ---
            word_index = tokenizer.word_index
            # Adjust vocab_size if the actual number of unique words is less than the defined vocab_size
            actual_vocab_size = min(vocab_size, len(word_index) + 1)
            embedding_matrix = np.zeros((actual_vocab_size, embedding_dim))
            for word, i in word_index.items():
                if i < actual_vocab_size:
                    if word in w2v_model.wv:
                        embedding_matrix[i] = w2v_model.wv[word]
                    else:
                        # Handle words not in W2V model (e.g., random initialization)
                        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))
            model = create_cnn_model_pretrained(vocab_size=len(word_index)+1, embedding_dim=100, embedding_matrix=embedding_matrix,
                                     max_length=100, num_classes=y_train.shape[1])

        else:
            bert_model_name = "cahya/distilbert-base-indonesian" if skenario["embedding"] == "indobert" else "distilbert-base-multilingual-cased"
            tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
            model_bert = TFDistilBertModel.from_pretrained(bert_model_name,from_pt=True)
            X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=32)
            X_val = bert_encode(texts_val, tokenizer, model_bert, max_len=100, batch_size=32)
            model = create_bert_cnn_model(input_shape=X_train.shape[1:], num_classes=y_train.shape[1])

        class_weights = compute_multilabel_class_weights(y_train)
        sample_weights = create_sample_weights(y_train, class_weights)

        history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                            sample_weight=sample_weights,
                            epochs=10, batch_size=64, verbose=0)

        thresholds = np.array([0.7, 0.5, 0.4, 0.4])
        y_pred = model.predict(X_val)
        y_pred_bin = (y_pred > thresholds).astype(int)

        # Accuracy per label
        per_label_acc = np.mean(y_val == y_pred_bin, axis=0)
        for i, label in enumerate(label_cols):
          print(f"Accuracy for {label}: {per_label_acc[i]:.4f}")

        # Rata-rata accuracy per label
        avg_label_acc = np.mean(per_label_acc)
        print(f"Average per-label accuracy: {avg_label_acc:.4f}")

        acc = avg_label_acc
        f1 = f1_score(y_val, y_pred_bin, average='macro')
        precision = precision_score(y_val, y_pred_bin, average='macro', zero_division=0)
        recall = recall_score(y_val, y_pred_bin, average='macro', zero_division=0)

        accs.append(acc)
        f1s.append(f1)
        precisions.append(precision)
        recalls.append(recall)

        # === Classification Report
        print("\nClassification Report:")
        print(classification_report(y_val, y_pred_bin, target_names=mlb.classes_, zero_division=0))


        # === ROC AUC Score per label
        try:
            roc_auc = roc_auc_score(y_val, y_pred, average=None)
            for i, label in enumerate(mlb.classes_):
                print(f"ROC AUC for {label}: {roc_auc[i]:.4f}")
        except ValueError as e:
            print("ROC AUC warning:", str(e))

        # === Confusion Matrix for each label
        for i, label in enumerate(mlb.classes_):
            cm = confusion_matrix(y_val[:, i], y_pred_bin[:, i])
            plt.figure(figsize=(4, 3))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title(f'Confusion Matrix: {label} (Fold {fold+1})')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            plt.show()


        from sklearn.metrics import roc_curve

        # === ROC Curve per label
        for i, label in enumerate(mlb.classes_):
            fpr, tpr, thresh = roc_curve(y_val[:, i], y_pred[:, i])

            plt.figure()
            plt.plot(fpr, tpr, label=f'AUC = {roc_auc[i]:.4f}')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve: {label} (Fold {fold+1})')
            plt.legend(loc='lower right')
            plt.grid(True)
            plt.show()

        # === Plot Acc & Loss
        plt.figure(figsize=(10, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='Train Acc')
        plt.plot(history.history['val_accuracy'], label='Val Acc')
        plt.title(f'Fold {fold+1} Accuracy')
        plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.ylim(0, 1); plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Val Loss')
        plt.title(f'Fold {fold+1} Loss')
        plt.xlabel('Epoch'); plt.ylabel('Loss');plt.legend()

        plt.tight_layout()
        plt.show()

    # Final Summary
    print("\n===== Cross-Validation Summary =====")
    print(f"Average Accuracy: {np.mean(accs):.4f}")
    print(f"Average F1 Score : {np.mean(f1s):.4f}")
    print(f"Average Precision: {np.mean(precisions):.4f}")
    print(f"Average Recall   : {np.mean(recalls):.4f}")
    print(f"Average F1 Score : {np.mean(f1s):.4f}")

    if skenario["embedding"] == "word2vec":
        return model, tokenizer, w2v_model
    else:
        return model, tokenizer, model_bert

#class_weight, l2, 3 pool
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_cnn_model(vocab_size, embedding_dim, max_length, num_classes, l2_strength=0.001):
    input_layer = Input(shape=(max_length,))
    embedding = Embedding(vocab_size, embedding_dim)(input_layer)

    conv_3 = Conv1D(96, 3, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(96, 4, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(96, 5, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_4 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4])
    drop = Dropout(0.3)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0003)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Reshape, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001):
    input_layer = Input(shape=input_shape)  # biasanya (768, )

    # Reshape supaya bisa Conv1D (menjadi (seq_len, channels=1))
    reshaped = Reshape((input_shape[0], 1))(input_layer)

    conv_3 = Conv1D(128, 3, activation='relu')(reshaped)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(128, 4, activation='relu')(reshaped)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(128, 5, activation='relu')(reshaped)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(0.3)(merged)

    dense = Dense(256, activation='relu')(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0003)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

def compute_multilabel_class_weights(y):
    # y shape = (samples, labels)
    class_weights = {}
    for i in range(y.shape[1]):
        classes = np.unique(y[:, i])
        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y[:, i])
        # Misal class 0 weight dan class 1 weight
        class_weights[i] = dict(zip(classes, weights))
    return class_weights

def create_sample_weights(y, class_weights):
    sample_weights = np.ones((y.shape[0],))
    # Contoh sederhana: untuk setiap label, tambahkan bobot jika labelnya 1
    for i in range(y.shape[1]):
        sample_weights += y[:, i] * class_weights[i][1]  # bobot kelas minoritas
    return sample_weights

bert_model_name = "cahya/distilbert-base-indonesian"
tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
model_bert = TFDistilBertModel.from_pretrained(bert_model_name,from_pt=True)
texts_train = df_train['cleaned_text'].astype(str)
X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=32)

w2v_model = Word2Vec([text.split() for text in texts_train], vector_size=100, min_count=1, sg=1)
word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}
tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
X_train = tokenize_word2vec(texts_train, tokenizer)

X_train

"""##Main Pipeline"""

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import KFold

def run_pipeline(data, skenario, num_folds=5):
    texts = data['stemmed_text'] if skenario['stem'] else data['cleaned_text']
    label_cols = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']
    mlb = MultiLabelBinarizer()
    mlb.classes_ = label_cols
    y = data[label_cols].values

    # K-Fold
    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)
    accs, f1s, precisions, recalls = [], [], [], []

    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):
        print(f"\n===== Fold {fold + 1} =====")

        df_train_fold = data.iloc[train_idx].reset_index(drop=True)
        df_val_fold = data.iloc[val_idx].reset_index(drop=True)

        text_col = 'stemmed_text' if skenario['stem'] else 'cleaned_text'

        # Oversampling pada data mentah train fold
        if skenario["oversample"]:
            df_train_fold = oversample(df_train_fold, text_col, label_cols)
            print(df_train_fold['aspect_attraction'].value_counts())
            print(df_train_fold['aspect_amenities'].value_counts())
            print(df_train_fold['aspect_access'].value_counts())
            print(df_train_fold['aspect_price'].value_counts())

        texts_train = df_train_fold[text_col].astype(str)
        y_train = df_train_fold[label_cols].values

        texts_val = df_val_fold[text_col].astype(str)
        y_val = df_val_fold[label_cols].values

        # Embedding per fold
        if skenario["embedding"] == "word2vec":
            w2v_model = Word2Vec([text.split() for text in texts_train], vector_size=100, min_count=1, sg=1)
            word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}
            tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
            X_train = tokenize_word2vec(texts_train, tokenizer)
            X_val = tokenize_word2vec(texts_val, tokenizer)
            model = create_cnn_model(vocab_size=len(word_index)+1, embedding_dim=100,
                                     max_length=100, num_classes=y_train.shape[1])

        else:
            bert_model_name = "cahya/distilbert-base-indonesian" if skenario["embedding"] == "indobert" else "distilbert-base-multilingual-cased"
            tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
            model_bert = TFDistilBertModel.from_pretrained(bert_model_name,from_pt=True)
            X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=32)
            X_val = bert_encode(texts_val, tokenizer, model_bert, max_len=100, batch_size=32)
            model = create_bert_cnn_model(input_shape=X_train.shape[1:], num_classes=y_train.shape[1])

        class_weights = compute_multilabel_class_weights(y_train)
        sample_weights = create_sample_weights(y_train, class_weights)

        history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                            sample_weight=sample_weights,
                            epochs=10, batch_size=64, verbose=0)

        thresholds = np.array([0.7, 0.5, 0.4, 0.4])
        y_pred = model.predict(X_val)
        y_pred_bin = (y_pred > thresholds).astype(int)

        # Accuracy per label
        per_label_acc = np.mean(y_val == y_pred_bin, axis=0)
        for i, label in enumerate(label_cols):
          print(f"Accuracy for {label}: {per_label_acc[i]:.4f}")

        # Rata-rata accuracy per label
        avg_label_acc = np.mean(per_label_acc)
        print(f"Average per-label accuracy: {avg_label_acc:.4f}")

        acc = avg_label_acc
        f1 = f1_score(y_val, y_pred_bin, average='macro')
        precision = precision_score(y_val, y_pred_bin, average='macro', zero_division=0)
        recall = recall_score(y_val, y_pred_bin, average='macro', zero_division=0)

        accs.append(acc)
        f1s.append(f1)
        precisions.append(precision)
        recalls.append(recall)

        # === Classification Report
        print("\nClassification Report:")
        print(classification_report(y_val, y_pred_bin, target_names=mlb.classes_, zero_division=0))


        # === ROC AUC Score per label
        try:
            roc_auc = roc_auc_score(y_val, y_pred, average=None)
            for i, label in enumerate(mlb.classes_):
                print(f"ROC AUC for {label}: {roc_auc[i]:.4f}")
        except ValueError as e:
            print("ROC AUC warning:", str(e))

        # === Confusion Matrix for each label
        for i, label in enumerate(mlb.classes_):
            cm = confusion_matrix(y_val[:, i], y_pred_bin[:, i])
            plt.figure(figsize=(4, 3))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title(f'Confusion Matrix: {label} (Fold {fold+1})')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            plt.show()


        from sklearn.metrics import roc_curve

        # === ROC Curve per label
        for i, label in enumerate(mlb.classes_):
            fpr, tpr, thresh = roc_curve(y_val[:, i], y_pred[:, i])

            plt.figure()
            plt.plot(fpr, tpr, label=f'AUC = {roc_auc[i]:.4f}')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve: {label} (Fold {fold+1})')
            plt.legend(loc='lower right')
            plt.grid(True)
            plt.show()

        # === Plot Acc & Loss
        plt.figure(figsize=(10, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='Train Acc')
        plt.plot(history.history['val_accuracy'], label='Val Acc')
        plt.title(f'Fold {fold+1} Accuracy')
        plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.ylim(0, 1); plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Val Loss')
        plt.title(f'Fold {fold+1} Loss')
        plt.xlabel('Epoch'); plt.ylabel('Loss');plt.legend()

        plt.tight_layout()
        plt.show()

    # Final Summary
    print("\n===== Cross-Validation Summary =====")
    print(f"Average Accuracy: {np.mean(accs):.4f}")
    print(f"Average F1 Score : {np.mean(f1s):.4f}")
    print(f"Average Precision: {np.mean(precisions):.4f}")
    print(f"Average Recall   : {np.mean(recalls):.4f}")
    print(f"Average F1 Score : {np.mean(f1s):.4f}")

    if skenario["embedding"] == "word2vec":
        return model, tokenizer, w2v_model
    else:
        return model, tokenizer, model_bert

"""###bert baru"""

#terbaru, not reshape,  tidak hanya mengambil [CLS] token, tapi mengambil seluruh output sequence dari BERT.
skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#terbaru, not reshape,  tidak hanya mengambil [CLS] token, tapi mengambil seluruh output sequence dari BERT.
skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#terbaru, not reshape,  tidak hanya mengambil [CLS] token, tapi mengambil seluruh output sequence dari BERT.
skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#terbaru, not reshape,  tidak hanya mengambil [CLS] token, tapi mengambil seluruh output sequence dari BERT.
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#terbaru, not reshape,  tidak hanya mengambil [CLS] token, tapi mengambil seluruh output sequence dari BERT.
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#terbaru, not reshape,  tidak hanya mengambil [CLS] token, tapi mengambil seluruh output sequence dari BERT.
skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#terbaru, not reshape,  tidak hanya mengambil [CLS] token, tapi mengambil seluruh output sequence dari BERT.
skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

"""##word2vec"""

#class_weight
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#class_weight
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#class_weight, l2, 3 pool
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#class_weight, l2, 3 pool
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

"""##best model aspect"""

#class_weight, l2, 3 pool
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#non_class_weight
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#class_weight
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#non_class_weight
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#threshold+lr dense 96
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

run_pipeline(df_train, skenario)

#threshold+lr 0.0005 dense akhir 128
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

run_pipeline(df_train, skenario)

"""##stem"""

skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#l2 3 pool
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#l2 3 pool
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

"""##BERT"""

#l2
skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#non_sample weight l2
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#sample weight, fine-tuning, threshold
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#sample weight, fine-tuning, threshold, regularization
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#sample weight, fine-tuning, threshold, regularization
skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": False
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#sample weight, fine-tuning, threshold, regularization
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#sample weight, fine-tuning, threshold, regularization
skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

#sample weight, fine-tuning, threshold
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": True
}

model, tokenizer, w2v_model = run_pipeline(df_train, skenario)

"""#Sentimen"""

df_attraction = df[['cleaned_text', 'stemmed_text', 'sentiment_attraction']]
df_attraction = df_attraction[~df_attraction['sentiment_attraction'].astype(str).str.lower().isin(['none'])]
df_attraction

# Fitur dan label
X_input = df_attraction[['cleaned_text', 'stemmed_text']]
y_input = df_attraction[['sentiment_attraction']].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

df_train_attraction = pd.concat([
    X_train.reset_index(drop=True),
    pd.DataFrame(y_train, columns=['sentiment_attraction'])
], axis=1)
df_train_attraction.dropna(inplace=True)
df_train_attraction.info()
df_train_attraction['sentiment_attraction'].value_counts()

df_test_attraction = pd.concat([
    X_test.reset_index(drop=True),
    pd.DataFrame(y_test, columns=['sentiment_attraction'])
], axis=1)
df_test_attraction.dropna(inplace=True)
df_test_attraction.info()
df_test_attraction['sentiment_attraction'].value_counts()

from imblearn.over_sampling import RandomOverSampler

def oversample(df, text_col, label_col):
    # Filter out rows where the label is neither 0 nor 1
    df_filtered = df[(df[label_col] == 0) | (df[label_col] == 1)].copy()

    X = df_filtered[[text_col]] # Keep text_col as a DataFrame/Series
    y = df_filtered[label_col].astype(int) # Ensure y is integer type

    ros = RandomOverSampler(random_state=42)

    # Resample the dataset
    # imblearn expects X as a 2D array, so we need to reshape or keep it as DataFrame
    X_resampled, y_resampled = ros.fit_resample(X, y)

    # Recreate the DataFrame
    df_resampled = pd.DataFrame(X_resampled, columns=[text_col])
    df_resampled[label_col] = y_resampled

    return df_resampled

def tokenize_word2vec(texts, tokenizer):
    """
    Tokenizes the input text using Word2Vec tokenizer.

    Args:
    texts (list of str): List of texts to be tokenized.
    tokenizer (function): A tokenizer function to convert words to indices.

    Returns:
    np.array: Tokenized texts in numerical form.
    """
    tokenized_texts = []
    for text in texts:
        tokenized_text = tokenizer(text)  # Use tokenizer to convert text to tokens
        tokenized_texts.append(tokenized_text)

    # Pad or truncate to max length (optional based on your requirement)
    max_len = 100  # Set max length of sequence (adjust as needed)
    tokenized_texts = [text + [0] * (max_len - len(text)) if len(text) < max_len else text[:max_len] for text in tokenized_texts]

    return np.array(tokenized_texts)

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel

def bert_encode(texts, tokenizer, model_bert, max_len=100, batch_size=32):
    outputs_all = []

    # Freeze all layers except embeddings
    for layer in model_bert.layers:
        layer.trainable = False
    model_bert.distilbert.embeddings.trainable = True  # Unfreeze embedding layer

    # Batch processing
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        encodings = tokenizer(
            batch_texts.tolist(),
            truncation=True,
            padding='max_length',
            max_length=max_len,
            return_tensors="tf"
        )
        outputs = model_bert(encodings['input_ids'], attention_mask=encodings['attention_mask'])
        last_hidden = outputs.last_hidden_state[:, 0, :]
        outputs_all.append(last_hidden.numpy())

    return np.vstack(outputs_all)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_cnn_model(vocab_size, embedding_dim, max_length, num_classes, l2_strength=0.001):
    input_layer = Input(shape=(max_length,))
    embedding = Embedding(vocab_size, embedding_dim)(input_layer)

    conv_3 = Conv1D(96, 3, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(96, 4, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(96, 5, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(0.3)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0001)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Reshape, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001):
    input_layer = Input(shape=input_shape)  # biasanya (768, )

    # Reshape supaya bisa Conv1D (menjadi (seq_len, channels=1))
    reshaped = Reshape((input_shape[0], 1))(input_layer)

    conv_3 = Conv1D(128, 3, activation='relu', kernel_regularizer=l2(l2_strength))(reshaped)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(128, 4, activation='relu', kernel_regularizer=l2(l2_strength))(reshaped)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(128, 5, activation='relu', kernel_regularizer=l2(l2_strength))(reshaped)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(0.3)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0003)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

# --------------------- Word2Vec Tokenization ---------------------
def tokenize_word2vec(texts, tokenizer, max_len=100):
    sequences = [tokenizer(text) for text in texts]
    return pad_sequences(sequences, maxlen=max_len, padding='post')

"""##Main Pipeline"""

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import KFold
from transformers import DistilBertTokenizer, TFDistilBertModel, BertTokenizer, TFBertModel
import numpy as np
from sklearn.utils.class_weight import compute_class_weight

def run_pipeline(data, skenario, label_col, num_folds=5, threshold=0.5):
    texts = data['stemmed_text'] if skenario['stem'] else data['cleaned_text']
    y = data[label_col].astype(int)

    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)
    accs, f1s, precisions, recalls, aucs = [], [], [], [], []

    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):
        print(f"\n===== Fold {fold + 1} =====")

        df_train_fold = data.iloc[train_idx].reset_index(drop=True)
        df_val_fold = data.iloc[val_idx].reset_index(drop=True)

        text_col = 'stemmed_text' if skenario['stem'] else 'cleaned_text'

        # Oversampling pada data train fold
        if skenario["oversample"]:
            df_train_fold = oversample(df_train_fold, text_col, label_col)
            print(df_train_fold[label_col].value_counts())

        texts_train = df_train_fold[text_col].astype(str)
        y_train = df_train_fold[label_col].values.astype(int)

        texts_val = df_val_fold[text_col].astype(str)
        y_val = df_val_fold[label_col].values.astype(int)

        print(f"Training data shape: {texts_train.shape}")
        print(f"Validation data shape: {texts_val.shape}")
        print(df_train_fold[label_col].value_counts())

        # Embedding per fold
        if skenario["embedding"] == "word2vec":
            w2v_model = Word2Vec([text.split() for text in texts_train], vector_size=100, min_count=1, sg=1)
            word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}
            tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
            X_train = tokenize_word2vec(texts_train, tokenizer)
            X_val = tokenize_word2vec(texts_val, tokenizer)
            print(f"X_train shape: {X_train.shape}")
            print(f"X_val shape: {X_val.shape}")
            model = create_cnn_model(vocab_size=len(word_index)+1, embedding_dim=100,
                                     max_length=100, num_classes=1) # Output hanya 1 untuk binary classification

        else:
            bert_model_name = "cahya/distilbert-base-indonesian" if skenario["embedding"] == "indobert" else "distilbert-base-multilingual-cased"
            tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
            model_bert = TFDistilBertModel.from_pretrained(bert_model_name, from_pt=True)
            X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=8)
            X_val = bert_encode(texts_val, tokenizer, model_bert, max_len=100, batch_size=8)
            model = create_bert_cnn_model(input_shape=X_train.shape[1:], num_classes=1)  # Output 1 unit untuk binary classification

        history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                            epochs=10, batch_size=64, verbose=0)

        y_pred = model.predict(X_val)
        y_pred_bin = (y_pred > threshold).astype(int)

        # Accuracy for binary classification
        acc = accuracy_score(y_val, y_pred_bin)
        f1 = f1_score(y_val, y_pred_bin, average='macro')
        precision = precision_score(y_val, y_pred_bin, average='macro', zero_division=0)
        recall = recall_score(y_val, y_pred_bin, average='macro', zero_division=0)

        accs.append(acc)
        f1s.append(f1)
        precisions.append(precision)
        recalls.append(recall)

        print(f"Accuracy: {acc:.4f} | F1: {f1:.4f}")

        # === Classification Report
        print("\nClassification Report:")
        print(classification_report(y_val, y_pred_bin, target_names=['negative', 'positive']))

        # === ROC AUC Score
        try:
            roc_auc = roc_auc_score(y_val, y_pred)
            aucs.append(roc_auc)
            print(f"ROC AUC: {roc_auc:.4f}")
        except ValueError as e:
            print("ROC AUC warning:", str(e))

        # === Confusion Matrix
        cm = confusion_matrix(y_val, y_pred_bin)
        plt.figure(figsize=(4, 3))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix (Fold {fold+1})')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.show()

        # === ROC Curve
        fpr, tpr, thresh = roc_curve(y_val, y_pred)

        plt.figure()
        plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve (Fold {fold+1})')
        plt.legend(loc='lower right')
        plt.grid(True)
        plt.show()

        # === Plot Acc & Loss
        plt.figure(figsize=(10, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='Train Acc')
        plt.plot(history.history['val_accuracy'], label='Val Acc')
        plt.title(f'Fold {fold+1} Accuracy')
        plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.ylim(0, 1); plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Val Loss')
        plt.title(f'Fold {fold+1} Loss')
        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()

        plt.tight_layout()
        plt.show()

    # Final Summary
    print("\n===== Cross-Validation Summary =====")
    print(f"Average Accuracy: {np.mean(accs):.4f}")
    print(f"Average F1 Score : {np.mean(f1s):.4f}")
    print(f"Average Precision: {np.mean(precisions):.4f}")
    print(f"Average Recall   : {np.mean(recalls):.4f}")
    print(f"Average ROC AUC  : {np.mean(aucs):.4f}")

    return model

"""##attraction"""

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.75)

#lr 01, l2, 2 pool
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.75)

#lr 01, l2, 2 pool
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.75)

#lr 01, l2, 2 pool
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.75)

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool, tr 0.75
skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_attraction'
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7)

# Split df_attraction into training and testing sets
X_attraction = df_attraction[['cleaned_text', 'stemmed_text']]
y_attraction = df_attraction['sentiment_attraction'].values

X_train_attraction, X_test_attraction, y_train_attraction, y_test_attraction = train_test_split(
    X_attraction, y_attraction, test_size=0.1, random_state=42, stratify=y_attraction
)

# Display the value counts for the target variable in the training set to check stratification
print("\nValue counts for y_train_attraction:")
print(pd.Series(y_train_attraction).value_counts())

# Display the value counts for the target variable in the testing set
print("\nValue counts for y_test_attraction:")
print(pd.Series(y_test_attraction).value_counts())

df_train_attraction = pd.concat([X_train_attraction.reset_index(drop=True), pd.Series(y_train_attraction, name='sentiment_attraction')], axis=1)
df_train_attraction

"""##amenities"""

df_amenities = df[['cleaned_text', 'stemmed_text', 'sentiment_amenities']]
df_amenities = df_amenities[~df_amenities['sentiment_amenities'].astype(str).str.lower().isin(['none'])]
df_amenities.info()

# Fitur dan label
X_input = df_amenities[['cleaned_text', 'stemmed_text']]
y_input = df_amenities[['sentiment_amenities']].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

df_train_amenities = pd.concat([
    X_train.reset_index(drop=True),
    pd.DataFrame(y_train, columns=['sentiment_amenities'])
], axis=1)
df_train_amenities.dropna(inplace=True)
df_train_amenities.info()
df_train_amenities['sentiment_amenities'].value_counts()

df_test_amenities = pd.concat([
    X_test.reset_index(drop=True),
    pd.DataFrame(y_test, columns=['sentiment_amenities'])
], axis=1)
df_test_amenities.dropna(inplace=True)
df_test_amenities.info()
df_test_amenities['sentiment_amenities'].value_counts()

skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.7)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.65)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.65)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.65)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.65)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.65)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.65)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.65)

#lr 01, l2, 2 pool,
skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_amenities'
model = run_pipeline(df_train_amenities, skenario, label_col, threshold=0.65)

"""##access"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_cnn_model(vocab_size, embedding_dim, max_length, num_classes, l2_strength=0.001):
    input_layer = Input(shape=(max_length,))
    embedding = Embedding(vocab_size, embedding_dim)(input_layer)

    conv_3 = Conv1D(96, 3, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(96, 4, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(96, 5, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4])
    drop = Dropout(0.3)(merged)

    dense = Dense(128, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0003)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

df_access = df[['cleaned_text', 'stemmed_text', 'sentiment_access']]
df_access = df_access[~df_access['sentiment_access'].astype(str).str.lower().isin(['none'])]
df_access.info()
df_access['sentiment_access'].value_counts()

# Fitur dan label
X_input = df_access[['cleaned_text', 'stemmed_text']]
y_input = df_access[['sentiment_access']].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

df_train_access = pd.concat([
    X_train.reset_index(drop=True),
    pd.DataFrame(y_train, columns=['sentiment_access'])
], axis=1)
df_train_access.dropna(inplace=True)
df_train_access.info()
df_train_access['sentiment_access'].value_counts()

df_test_access = pd.concat([
    X_test.reset_index(drop=True),
    pd.DataFrame(y_test, columns=['sentiment_access'])
], axis=1)
df_test_access.dropna(inplace=True)
df_test_access.info()
df_test_access['sentiment_access'].value_counts()

skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_access'
model = run_pipeline(df_train_access, skenario, label_col)

"""##price"""

df_price = df[['cleaned_text', 'stemmed_text', 'sentiment_price']]
df_price = df_price[~df_price['sentiment_price'].astype(str).str.lower().isin(['none'])]
df_price.info()
df_price['sentiment_price'].value_counts()

# Fitur dan label
X_input = df_price[['cleaned_text', 'stemmed_text']]
y_input = df_price[['sentiment_price']].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.1, random_state=42)

df_train_price = pd.concat([
    X_train.reset_index(drop=True),
    pd.DataFrame(y_train, columns=['sentiment_price'])
], axis=1)
df_train_price.dropna(inplace=True)
df_train_price.info()
df_train_price['sentiment_price'].value_counts()

df_test_price = pd.concat([
    X_test.reset_index(drop=True),
    pd.DataFrame(y_test, columns=['sentiment_price'])
], axis=1)
df_test_price.dropna(inplace=True)
df_test_price.info()
df_test_price['sentiment_price'].value_counts()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_cnn_model(vocab_size, embedding_dim, max_length, num_classes, l2_strength=0.001):
    input_layer = Input(shape=(max_length,))
    embedding = Embedding(vocab_size, embedding_dim)(input_layer)

    conv_3 = Conv1D(96, 3, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(96, 4, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(96, 5, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4])
    drop = Dropout(0.5)(merged)

    dense = Dense(128, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0003)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "word2vec",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "word2vec",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "bert",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "bert",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "indobert",
    "stem": False,
    "oversample": True
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": False
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

skenario = {
    "embedding": "indobert",
    "stem": True,
    "oversample": True
}

label_col = 'sentiment_price'
model = run_pipeline(df_train_price, skenario, label_col, threshold=0.5)

"""# Re-Train"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001, filters=128, learning_rate=0.0003, dropout_rate=0.3):
    input_layer = Input(shape=input_shape)  # now input_shape = (100, 768)

    conv_3 = Conv1D(filters, 3, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(filters, 4, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(filters, 5, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(dropout_rate)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

"""##Aspect"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_cnn_model(vocab_size, embedding_dim, embedding_matrix, max_length, num_classes, l2_strength=0.001, filters=256, learning_rate=0.001, dropout_rate=0.4):
    input_layer = Input(shape=(max_length,))
    embedding = Embedding(
        input_dim=vocab_size,
        output_dim=embedding_dim,
        weights=[embedding_matrix],
        input_length=max_length,
        trainable=False
    )(input_layer)

    conv_3 = Conv1D(filters, 3, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(filters, 4, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(filters, 5, activation='relu', kernel_regularizer=l2(l2_strength))(embedding)
    pool_4 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4])
    drop = Dropout(dropout_rate)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

from sklearn.preprocessing import MultiLabelBinarizer
import numpy as np
import tensorflow as tf
from gensim.models import Word2Vec
import os
import json

def run_pipeline_default(train_data, save_path='model_saved'):
    texts_train = train_data['stemmed_text'].astype(str)
    label_cols = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']

    mlb = MultiLabelBinarizer()
    mlb.classes_ = label_cols
    y_train = train_data[label_cols].values.astype(int)

    # Train Word2Vec
    w2v_model = Word2Vec([text.split() for text in texts_train], vector_size=100, min_count=1, sg=1)
    word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}
    tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
    X_train = tokenize_word2vec(texts_train, tokenizer)

    class_weights = compute_multilabel_class_weights(y_train)
    sample_weights = create_sample_weights(y_train, class_weights)

    # Gunakan model CNN buatanmu
    model = create_cnn_model(
        vocab_size=len(word_index) + 1,
        embedding_dim=100,
        max_length=100,
        num_classes=4,
        filters=128,
        learning_rate=0.0003,
        dropout_rate=0.3,
        l2_strength=0.001
    )

    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3)

    model.fit(
        X_train, y_train,
        sample_weight=sample_weights,
        epochs=10,
        batch_size=64,
        validation_split=0.1,
        callbacks=[early_stopping]
    )

    # Simpan semua file penting
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    model.save(f'{save_path}/model_final.h5')

    with open(f'{save_path}/word_index.json', 'w') as f:
        json.dump(word_index, f)

    w2v_model.save(f'{save_path}/word2vec.model')
    embedding_matrix = w2v_model.wv.vectors
    np.save(f'{save_path}/embedding_matrix.npy', embedding_matrix)

    with open(f'{save_path}/config.json', 'w') as f:
        json.dump({'max_len': 100}, f)

    return

run_pipeline_default(df_train, 'aspect_model')

import numpy as np
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.sequence import pad_sequences
import json
import keras_tuner as kt
import tensorflow as tf
from tensorflow.keras.optimizers import Adam

# Fungsi tokenisasi dan padding
def tokenize_word2vec(texts, tokenizer, max_len=100):
    sequences = [tokenizer(text) for text in texts]
    return pad_sequences(sequences, maxlen=max_len, padding='post')

def create_cnn_model(vocab_size, embedding_dim, max_length, num_classes, embedding_matrix,
                     filters=128, learning_rate=1e-4, dropout_rate=0.3, l2_strength=0.001):
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
    from tensorflow.keras.regularizers import l2

    input_layer = Input(shape=(max_length,))
    embedding_layer = Embedding(
        input_dim=vocab_size,
        output_dim=embedding_dim,
        weights=[embedding_matrix],
        input_length=max_length,
        trainable=False
    )(input_layer)

    conv_3 = Conv1D(filters, 3, activation='relu', kernel_regularizer=l2(l2_strength))(embedding_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(filters, 4, activation='relu', kernel_regularizer=l2(l2_strength))(embedding_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(filters, 5, activation='relu', kernel_regularizer=l2(l2_strength))(embedding_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(dropout_rate)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

def run_pipeline_with_tuning(train_data, save_path='model_saved'):
    texts_train = train_data['stemmed_text']
    label_cols = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']
    y_train = train_data[label_cols].values.astype(int)

    # Latih Word2Vec
    w2v_model = Word2Vec([text.split() for text in texts_train], vector_size=100, min_count=1, sg=1)
    word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}

    # Buat embedding matrix sinkron dengan word_index
    embedding_dim = w2v_model.vector_size
    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
    for word, i in word_index.items():
        if word in w2v_model.wv:
            embedding_matrix[i] = w2v_model.wv[word]

    # Tokenisasi dan padding input teks
    tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
    X_train = tokenize_word2vec(texts_train, tokenizer)

    # Jika kamu punya fungsi compute_multilabel_class_weights dan create_sample_weights, pakai di sini
    class_weights = compute_multilabel_class_weights(y_train)
    sample_weights = create_sample_weights(y_train, class_weights)

    # Fungsi tuning model
    def model_builder(hp):
        model = create_cnn_model(
            vocab_size=len(word_index) + 1,
            embedding_dim=embedding_dim,
            max_length=100,
            num_classes=len(label_cols),
            embedding_matrix=embedding_matrix,
            filters=hp.Int('filters', min_value=64, max_value=256, step=32),
            learning_rate=hp.Float('learning_rate', 1e-5, 1e-3, sampling='log'),
            dropout_rate=hp.Float('dropout_rate', 0.2, 0.5, step=0.1)
        )
        return model

    tuner = kt.BayesianOptimization(
        model_builder,
        objective='val_loss',
        max_trials=10,
        directory='tuning_results',
        project_name='model_tuning'
    )

    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min')

    tuner.search(X_train, y_train, sample_weight=sample_weights, epochs=10, batch_size=64,
                 validation_split=0.1, callbacks=[early_stopping])

    best_model = tuner.get_best_models(num_models=1)[0]

    # Simpan model terbaik dan artefak lainnya
    os.makedirs(save_path, exist_ok=True)
    best_model.save(f'{save_path}/model_final.h5')

    with open(f'{save_path}/word_index.json', 'w') as f:
        json.dump(word_index, f)

    w2v_model.save(f'{save_path}/word2vec.model')
    np.save(f'{save_path}/embedding_matrix.npy', embedding_matrix)

    with open(f'{save_path}/config.json', 'w') as f:
        json.dump({'max_len': 100}, f)

    print("Pipeline selesai dan model tersimpan di:", save_path)

run_pipeline_with_tuning(df_train, save_path='aspect_tuning')

model.summary()

run_pipeline_with_tuning(df_train, save_path='aspect_tuning')

## SENTIMENT

import keras_tuner as kt
from tensorflow.keras.optimizers import Adam

# Fungsi untuk menjalankan pipeline dengan tuning hyperparameter
def run_pipeline_with_tuning(train_data, label_col, threshold=0.5, save_path='model_saved'):

    texts_train = train_data['cleaned_text']
    y_train = train_data[label_col].astype(int)

    bert_model_name = "cahya/distilbert-base-indonesian"
    model_bert = TFDistilBertModel.from_pretrained(bert_model_name, from_pt=True)
    model_bert.save_pretrained(f'{save_path}/model_bert')
    tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
    X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=16)

    # Fungsi untuk tunning hyperparameter
    def model_builder(hp):
        model = create_bert_cnn_model(input_shape=X_train.shape[1:], num_classes=1,
                                      filters=hp.Int('filters', min_value=64, max_value=256, step=32),
                                      #kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1),
                                      learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),
                                      dropout_rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1))
        return model

    # Tuning menggunakan Keras Tuner
    tuner = kt.BayesianOptimization(model_builder,
                                    objective='val_loss',  # Tuning berdasarkan f1_macro
                                    max_trials=10,
                                    directory='tuning_results',
                                    project_name='model_tuning')

    # Tentukan callback untuk early stopping
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3)

    # Fit tuner
    tuner.search(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1, callbacks=[early_stopping])

    # Ambil model terbaik setelah tuning
    best_model = tuner.get_best_models(num_models=1)[0]

    # Cetak parameter terbaik
    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]
    print("Best Hyperparameters:")
    print(f"Filters: {best_trial.hyperparameters['filters']}")
    #print(f"Kernel Size: {best_trial.hyperparameters['kernel_size']}")
    print(f"Learning Rate: {best_trial.hyperparameters['learning_rate']}")
    print(f"Dropout Rate: {best_trial.hyperparameters['dropout_rate']}")

    # Simpan model dan tokenizer
    best_model.save(f'{save_path}/model_final.h5')  # Simpan model terbaik
    tokenizer.save_pretrained(f'{save_path}/tokenizer_final')  # Simpan tokenizer

    return best_model, tokenizer

from transformers import DistilBertTokenizer, TFDistilBertModel
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
import os

def run_pipeline_default(train_data, label_col, threshold=0.5, save_path='model_saved'):
    texts_train = train_data['cleaned_text'].fillna('').astype(str)
    y_train = train_data[label_col].astype(int)

    # Load tokenizer dan model pre-trained DistilBERT Indonesia
    bert_model_name = "cahya/distilbert-base-indonesian"
    tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
    model_bert = TFDistilBertModel.from_pretrained(bert_model_name, from_pt=True)

    # Encode teks menggunakan BERT
    X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=16)

    # Gunakan model BERT + CNN buatanmu (tanpa tuning)
    model = create_bert_cnn_model(
        input_shape=X_train.shape[1:],
        num_classes=1,
        filters=128,
        learning_rate=3e-4,
        dropout_rate=0.3
    )

    # Early stopping
    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # Fit model
    model.fit(
        X_train, y_train,
        epochs=10,
        batch_size=64,
        validation_split=0.1,
        callbacks=[early_stopping]
    )

    # Simpan model dan tokenizer
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    model.save(f'{save_path}/price_final.h5')

    return model, tokenizer

run_pipeline_default(df_train_attraction, 'sentiment_attraction', threshold=0.7, save_path='attraction_tuning')

"""##attraction"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001, filters=128, learning_rate=0.0006759509436305564, dropout_rate=0.4):
    input_layer = Input(shape=input_shape)  # now input_shape = (100, 768)

    conv_3 = Conv1D(filters, 3, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(filters, 4, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(filters, 5, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(dropout_rate)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

model, tokenizer = run_pipeline_with_tuning(df_train_attraction, 'sentiment_attraction', threshold=0.7, save_path='attraction_tuning')

"""##amenities"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001, filters=96, learning_rate=0.0008961015644502074, dropout_rate=0.2):
    input_layer = Input(shape=input_shape)  # now input_shape = (100, 768)

    conv_3 = Conv1D(filters, 3, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(filters, 4, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(filters, 5, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(dropout_rate)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

run_pipeline_default(df_train_amenities, label_col='sentiment_amenities', threshold=0.5, save_path='amenities_tuning')

model, tokenizer = run_pipeline_with_tuning(df_train_amenities, label_col='sentiment_amenities', threshold=0.5, save_path='amenities_tuning')

"""##access"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001, filters=64, learning_rate=4.837200625807212e-05, dropout_rate=0.30000000000000004):
    input_layer = Input(shape=input_shape)  # now input_shape = (100, 768)

    conv_3 = Conv1D(filters, 3, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(filters, 4, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(filters, 5, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(dropout_rate)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

run_pipeline_default(df_train_access, label_col='sentiment_access', threshold=0.5, save_path='access_tuning')

model, tokenizer = run_pipeline_with_tuning(df_train_access, label_col='sentiment_access', threshold=0.5, save_path='access_tuning')

"""##price"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001, filters=64, learning_rate=0.0009226604447116868, dropout_rate=0.2):
    input_layer = Input(shape=input_shape)  # now input_shape = (100, 768)

    conv_3 = Conv1D(filters, 3, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(filters, 4, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(filters, 5, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(dropout_rate)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

run_pipeline_default(df_train_price, label_col='sentiment_price', threshold=0.5, save_path='price_tuning')

model, tokenizer = run_pipeline_with_tuning(df_train_price, label_col='sentiment_price', threshold=0.5, save_path='price_tuning')

# Preprocessing data berdasarkan skenario
def preprocess_data(train_data, skenario):
    texts = train_data['cleaned_text'].tolist()
    if skenario["embedding"] == "word2vec":
        w2v_model = Word2Vec([text.split() for text in texts], vector_size=100, min_count=1, sg=1)
        word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}
        tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
        X = tokenize_word2vec(texts, tokenizer)
        return X, None, tokenizer  # Tidak ada attention mask
    else:
        bert_model_name = "cahya/distilbert-base-indonesian"
        tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
        encoding = tokenizer(texts, return_tensors="tf", padding=True, truncation=True, max_length=100)
        return [encoding['input_ids'], encoding['attention_mask']], tokenizer, None

def model_builder(hp):
    model = create_bert_cnn_model(
        input_shape=X_train.shape[1:],,
        num_classes=1,
        filters=hp.Int('filters', min_value=64, max_value=256, step=32),
        kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1),
        learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),
        dropout_rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)
    )
    return model

def run_pipeline_with_tuning(train_data, skenario, label_col, threshold=0.5, save_path='model_saved'):
    y_train = train_data[label_col].astype(int)

    X_train, tokenizer, _ = preprocess_data(train_data, skenario)

    tuner = kt.BayesianOptimization(
        model_builder,
        objective=kt.Objective('val_f1_macro', direction='max'),
        max_trials=10,
        directory='tuning_results',
        project_name='model_tuning'
    )

    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_macro', patience=3)

    tuner.search(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])

    best_model = tuner.get_best_models(num_models=1)[0]

    # Simpan hasil
    best_model.save(f'{save_path}/model_final.h5')
    if tokenizer:
        tokenizer.save_pretrained(f'{save_path}/tokenizer_final')

    return best_model, tokenizer

from tensorflow.keras.optimizers import Adam
import keras_tuner as kt
from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dropout, Dense, Concatenate
import numpy as np


# Fungsi untuk tunning hyperparameter
def model_builder(hp, skenario, tokenizer, texts_train):
    if skenario["embedding"] == "word2vec":
        # Model CNN untuk Word2Vec
        w2v_model = Word2Vec([text.split() for text in texts_train], vector_size=100, min_count=1, sg=1)
        word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}
        tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
        X_train = tokenize_word2vec(texts_train, tokenizer)
        model = create_bert_cnn_model(input_shape=(100, 100), num_classes=1,
                                      filters=hp.Int('filters', min_value=64, max_value=256, step=32),
                                      kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1),
                                      learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),
                                      dropout_rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1))
    else:
        # Model CNN untuk BERT/DistilBERT
        bert_model_name = "cahya/distilbert-base-indonesian"
        model_bert = TFDistilBertModel.from_pretrained(bert_model_name)
        X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=16)
        model = create_bert_cnn_model(input_shape=X_train.shape[1:], num_classes=1,
                                      filters=hp.Int('filters', min_value=64, max_value=256, step=32),
                                      kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1),
                                      learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),
                                      dropout_rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1))

    return model

# Fungsi untuk menjalankan pipeline dengan tuning hyperparameter
def run_pipeline_with_tuning(train_data, skenario, label_col, threshold=0.5, save_path='model_saved'):

    texts_train = train_data['cleaned_text']
    y_train = train_data[label_col].astype(int)

    # Tokenizer untuk BERT/DistilBERT
    bert_model_name = "cahya/distilbert-base-indonesian"
    tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
    bert_model_name.save_pretrained(f'{save_path}/bert_model')

    # Tokenisasi seluruh data
    encoding_train = tokenizer(texts_train.tolist(), return_tensors="tf", padding=True, truncation=True, max_length=100)
    input_ids_train = encoding_train['input_ids']
    attention_mask_train = encoding_train['attention_mask']

    # Fungsi untuk tunning hyperparameter
    def model_builder(hp):
        model = create_bert_cnn_model(input_shape=(100, 768), num_classes=1,
                                      filters=hp.Int('filters', min_value=64, max_value=256, step=32),
                                      kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1),
                                      learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),
                                      dropout_rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1))
        return model

    # Tuning menggunakan Keras Tuner
    tuner = kt.BayesianOptimization(model_builder,
                                    objective='val_loss',  # Tuning berdasarkan loss pada validation set
                                    max_trials=10,
                                    directory='tuning_results',
                                    project_name='model_tuning')

    # Tentukan callback untuk early stopping
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

    # Fit tuner
    tuner.search([input_ids_train, attention_mask_train], y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])

    # Ambil model terbaik setelah tuning
    best_model = tuner.get_best_models(num_models=1)[0]

    # Cetak parameter terbaik
    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]
    print("Best Hyperparameters:")
    print(f"Filters: {best_trial.hyperparameters['filters']}")
    print(f"Kernel Size: {best_trial.hyperparameters['kernel_size']}")
    print(f"Learning Rate: {best_trial.hyperparameters['learning_rate']}")
    print(f"Dropout Rate: {best_trial.hyperparameters['dropout_rate']}")

    # Simpan model dan tokenizer
    best_model.save(f'{save_path}/model_final.h5')  # Simpan model terbaik
    tokenizer.save_pretrained(f'{save_path}/tokenizer_final')  # Simpan tokenizer

    return best_model, tokenizer

# Contoh pemanggilan
skenario = {
    "embedding": "distilbert",  # Atau bisa diganti dengan "word2vec" untuk menggunakan word2vec
    "stem": False,
    "oversample": True
}

model, tokenizer = run_pipeline_with_tuning(df_train_attraction, skenario, 'sentiment_attraction', threshold=0.5, save_path='model_saved')

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

def create_bert_cnn_model(input_shape, num_classes, l2_strength=0.001):
    input_layer = Input(shape=input_shape)  # now input_shape = (100, 768)

    conv_3 = Conv1D(128, 3, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_3 = GlobalMaxPooling1D()(conv_3)

    conv_4 = Conv1D(128, 4, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_4 = GlobalMaxPooling1D()(conv_4)

    conv_5 = Conv1D(128, 5, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool_5 = GlobalMaxPooling1D()(conv_5)

    merged = Concatenate()([pool_3, pool_4, pool_5])
    drop = Dropout(0.3)(merged)

    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    opt = Adam(learning_rate=0.0003)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

from transformers import DistilBertTokenizer, TFDistilBertModel, BertTokenizer, TFBertModel
import numpy as np

def run_pipeline(train_data, skenario, label_col, threshold=0.5, save_path='model_saved'):

    texts_train = train_data['stemmed_text'] if skenario['stem'] else train_data['cleaned_text']
    y_train = train_data[label_col].astype(int)
    print(f"Training label distribution: {y_train.value_counts()}")

    # Embedding dan model
    if skenario["embedding"] == "word2vec":
        w2v_model = Word2Vec([text.split() for text in texts_train], vector_size=100, min_count=1, sg=1)
        word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}
        tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
        X_train = tokenize_word2vec(texts_train, tokenizer)
        model = create_cnn_model(vocab_size=len(word_index)+1, embedding_dim=100,
                                 max_length=100, num_classes=1)  # Output hanya 1 untuk binary classification

    else:
        bert_model_name = "cahya/distilbert-base-indonesian" if skenario["embedding"] == "indobert" else "distilbert-base-multilingual-cased"
        tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)
        model_bert = TFDistilBertModel.from_pretrained(bert_model_name, from_pt=True)
        X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=16)
        model = create_bert_cnn_model(input_shape=X_train.shape[1:], num_classes=1)  # Output 1 unit untuk binary classification

    # Melatih model menggunakan seluruh data (train)
    history = model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)

    # Simpan model dan tokenizer
    model.save(f'{save_path}/model_final.h5')  # Simpan model
    model_bert.save_pretrained(f'{save_path}/model_bert.h5')  # Simpan model bert
    tokenizer.save_pretrained(f'{save_path}/tokenizer_final')  # Simpan tokenizer

skenario = {
    "embedding": "bert",  # Ganti sesuai dengan skenario embedding yang digunakan
    "stem": False,
    "oversample": False
}

label_col = 'sentiment_attraction'  # Kolom label yang digunakan
model = run_pipeline(df_train_attraction, skenario, label_col, threshold=0.7, save_path='attraction2')

from transformers import DistilBertTokenizer, TFDistilBertModel
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dropout, Dense, Concatenate
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import keras_tuner as kt
import numpy as np

# Fungsi untuk membuat CNN model
def create_cnn_model(input_shape, num_classes, l2_strength=0.001, filters=128, kernel_size=3):
    input_layer = Input(shape=input_shape)
    conv = Conv1D(filters, kernel_size, activation='relu', kernel_regularizer=l2(l2_strength))(input_layer)
    pool = GlobalMaxPooling1D()(conv)
    drop = Dropout(0.3)(pool)
    dense = Dense(256, activation='relu', kernel_regularizer=l2(l2_strength))(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=input_layer, outputs=output)
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0003), metrics=['accuracy'])
    return model

# Fungsi untuk membuat model BERT + CNN
def create_bert_cnn_model(input_shape, num_classes, filters=128, kernel_size=3):
    input_ids = Input(shape=input_shape, dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=input_shape, dtype=tf.int32, name='attention_mask')

    model_bert = TFDistilBertModel.from_pretrained("distilbert-base-uncased")
    bert_output = model_bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state

    conv = Conv1D(filters, kernel_size, activation='relu')(bert_output)
    pool = GlobalMaxPooling1D()(conv)
    drop = Dropout(0.3)(pool)
    dense = Dense(256, activation='relu')(drop)
    output = Dense(num_classes, activation='sigmoid')(dense)

    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0003), metrics=['accuracy'])
    return model

# Fungsi untuk model builder dalam Keras Tuner
def model_builder(hp, skenario, tokenizer, texts_train):
    if skenario["embedding"] == "word2vec":
        # Model CNN untuk Word2Vec
        w2v_model = Word2Vec([text.split() for text in texts_train], vector_size=100, min_count=1, sg=1)
        word_index = {word: idx + 1 for idx, word in enumerate(w2v_model.wv.index_to_key)}
        tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]
        X_train = tokenize_word2vec(texts_train, tokenizer)
        model = create_cnn_model(input_shape=(100, 100), num_classes=1,
                                 filters=hp.Int('filters', min_value=64, max_value=256, step=32),
                                 kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1))
    else:
        # Model CNN untuk BERT/DistilBERT
        bert_model_name = "distilbert-base-uncased" if skenario["embedding"] == "distilbert" else "bert-base-multilingual-cased"
        model_bert = TFDistilBertModel.from_pretrained(bert_model_name)
        X_train = bert_encode(texts_train, tokenizer, model_bert, max_len=100, batch_size=16)
        model = create_bert_cnn_model(input_shape=X_train.shape[1:], num_classes=1,
                                      filters=hp.Int('filters', min_value=64, max_value=256, step=32),
                                      kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1))

    return model

# Fungsi untuk menjalankan pipeline dengan tuning hyperparameter
def run_pipeline_with_tuning(train_data, skenario, label_col, threshold=0.7, save_path='model_saved'):

    texts_train = train_data['stemmed_text'] if skenario['stem'] else train_data['cleaned_text']
    y_train = train_data[label_col].astype(int)

    # Tokenizer untuk BERT/DistilBERT
    bert_model_name = "cahya/distilbert-base-indonesian" if skenario["embedding"] == "indobert" else "distilbert-base-multilingual-cased"
    tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)

    # Tokenisasi seluruh data
    encoding_train = tokenizer(texts_train.tolist(), return_tensors="tf", padding=True, truncation=True, max_length=100)
    input_ids_train = encoding_train['input_ids']
    attention_mask_train = encoding_train['attention_mask']

    # Fungsi untuk tunning hyperparameter
    def model_builder(hp):
        if skenario["embedding"] == "word2vec":
            model = create_cnn_model(input_shape=(100, 100), num_classes=1,
                                     filters=hp.Int('filters', min_value=64, max_value=256, step=32),
                                     kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1))
        else:
            model = create_bert_cnn_model(input_shape=(100, 768), num_classes=1,
                                           filters=hp.Int('filters', min_value=64, max_value=256, step=32),
                                           kernel_size=hp.Int('kernel_size', min_value=3, max_value=5, step=1))
        return model

    # Tuning menggunakan Keras Tuner
    tuner = kt.BayesianOptimization(model_builder,
                                    objective='val_loss',  # Tuning berdasarkan loss pada validation set
                                    max_trials=10,
                                    directory='tuning_results',
                                    project_name='model_tuning')

    # Tentukan callback untuk early stopping
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

    # Fit tuner
    tuner.search([input_ids_train, attention_mask_train], y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])

    # Ambil model terbaik setelah tuning
    best_model = tuner.get_best_models(num_models=1)[0]

    # Cetak parameter terbaik
    best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]
    print("Best Hyperparameters:")
    print(f"Filters: {best_trial.hyperparameters['filters']}")
    print(f"Kernel Size: {best_trial.hyperparameters['kernel_size']}")

    # Simpan model dan tokenizer
    best_model.save(f'{save_path}/model_final.h5')  # Simpan model terbaik
    tokenizer.save_pretrained(f'{save_path}/tokenizer_final')  # Simpan tokenizer

    return best_model, tokenizer

skenario = {
    "embedding": "distilbert",  # Atau bisa diganti dengan "word2vec" untuk menggunakan word2vec
    "stem": False,
    "oversample": True
}

model, tokenizer = run_pipeline_with_tuning(df_train_attraction, skenario, 'sentiment_attraction', threshold=0.5, save_path='model_saved')

"""# Testing

##Aspect
"""

import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MultiLabelBinarizer

# === Load tokenizer (word_index) ===
with open("aspect_tuning/word_index.json", "r") as f:
    word_index = json.load(f)
word_index = {str(k): int(v) for k, v in word_index.items()}

tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]

# === Load CNN model ===
model_cnn = model
print(model_cnn.summary())

# === Tokenisasi untuk Word2Vec ===
def tokenize_word2vec(texts, tokenizer, max_len=100):
    sequences = [tokenizer(text) for text in texts]
    return pad_sequences(sequences, maxlen=max_len, padding='post')

# === Test data ===
texts_test = df_test['stemmed_text']
label_cols = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']
y_test = df_test[label_cols].values.astype(int)

mlb = MultiLabelBinarizer()
mlb.classes_ = label_cols

# === Preprocessing teks ===
X_test = tokenize_word2vec(texts_test, tokenizer, max_len=100)

# === Predict multilabel ===
y_pred_prob = model_cnn.predict(X_test)

# Threshold untuk multilabel (bisa disesuaikan per label)
thresholds = np.array([0.7, 0.5, 0.45, 0.4])
# Agar broadcast benar
thresholds = thresholds.reshape(1, -1)
y_pred_bin = (y_pred_prob > thresholds).astype(int)

# === Evaluasi multilabel ===

# Accuracy per label
for i, col in enumerate(label_cols):
    acc = accuracy_score(y_test[:, i], y_pred_bin[:, i])
    print(f"Accuracy {col}: {acc:.4f}")

print("\nClassification Report (multilabel):")
print(classification_report(y_test, y_pred_bin, target_names=label_cols))

# ROC AUC per label
try:
    aucs = []
    for i in range(y_test.shape[1]):
        auc = roc_auc_score(y_test[:, i], y_pred_prob[:, i])
        aucs.append(auc)
        print(f"ROC AUC {label_cols[i]}: {auc:.4f}")
except Exception as e:
    print("ROC AUC warning:", str(e))

# === ROC Curve per label ===
for i, label in enumerate(mlb.classes_):
    fpr, tpr, thresh = roc_curve(y_test[:, i], y_pred_prob[:, i])
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {aucs[i]:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve: {label}')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()

# Confusion matrix per label
for i, col in enumerate(label_cols):
    cm = confusion_matrix(y_test[:, i], y_pred_bin[:, i])
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix {col}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MultiLabelBinarizer

# === Load tokenizer (word_index) ===
with open("aspect_tuning/word_index.json", "r") as f:
    word_index = json.load(f)
word_index = {str(k): int(v) for k, v in word_index.items()}

tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]

# === Load CNN model ===
model_cnn = load_model("aspect_tuning/model_final.h5")
print(model_cnn.summary())

# === Tokenisasi untuk Word2Vec ===
def tokenize_word2vec(texts, tokenizer, max_len=100):
    sequences = [tokenizer(text) for text in texts]
    return pad_sequences(sequences, maxlen=max_len, padding='post')

# === Test data ===
texts_test = df_test['stemmed_text']
label_cols = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']
y_test = df_test[label_cols].values.astype(int)

mlb = MultiLabelBinarizer()
mlb.classes_ = label_cols

# === Preprocessing teks ===
X_test = tokenize_word2vec(texts_test, tokenizer, max_len=100)

# === Predict multilabel ===
y_pred_prob = model_cnn.predict(X_test)

# Threshold untuk multilabel (bisa disesuaikan per label)
thresholds = np.array([0.7, 0.5, 0.45, 0.4])
# Agar broadcast benar
thresholds = thresholds.reshape(1, -1)
y_pred_bin = (y_pred_prob > thresholds).astype(int)

# === Evaluasi multilabel ===

# Accuracy per label
for i, col in enumerate(label_cols):
    acc = accuracy_score(y_test[:, i], y_pred_bin[:, i])
    print(f"Accuracy {col}: {acc:.4f}")

print("\nClassification Report (multilabel):")
print(classification_report(y_test, y_pred_bin, target_names=label_cols))

# ROC AUC per label
try:
    aucs = []
    for i in range(y_test.shape[1]):
        auc = roc_auc_score(y_test[:, i], y_pred_prob[:, i])
        aucs.append(auc)
        print(f"ROC AUC {label_cols[i]}: {auc:.4f}")
except Exception as e:
    print("ROC AUC warning:", str(e))

# === ROC Curve per label ===
for i, label in enumerate(mlb.classes_):
    fpr, tpr, thresh = roc_curve(y_test[:, i], y_pred_prob[:, i])
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {aucs[i]:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve: {label}')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()

# Confusion matrix per label
for i, col in enumerate(label_cols):
    cm = confusion_matrix(y_test[:, i], y_pred_bin[:, i])
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix {col}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
from sklearn.preprocessing import MultiLabelBinarizer

# === Load tokenizer (word_index) ===
with open("aspect_tuning/word_index.json", "r") as f:
    word_index = json.load(f)
word_index = {str(k): int(v) for k, v in word_index.items()}

tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]

# === Load CNN model ===
model_cnn = load_model("aspect_tuning/model_final.h5")

# === Tokenisasi untuk Word2Vec ===
def tokenize_word2vec(texts, tokenizer, max_len=100):
    sequences = [tokenizer(text) for text in texts]
    return pad_sequences(sequences, maxlen=max_len, padding='post')

# === Test data ===
texts_test = df_test['stemmed_text']
label_cols = ['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']
y_test = df_test[label_cols].values.astype(int)

mlb = MultiLabelBinarizer()
mlb.classes_ = label_cols

# === Preprocessing teks ===
X_test = tokenize_word2vec(texts_test, tokenizer, max_len=100)

# === Predict multilabel ===
y_pred_prob = model_cnn.predict(X_test)

# Threshold untuk multilabel (bisa disesuaikan per label)
thresholds = np.array([0.7, 0.5, 0.4, 0.4])
# Agar broadcast benar
thresholds = thresholds.reshape(1, -1)
y_pred_bin = (y_pred_prob > thresholds).astype(int)

# === Evaluasi multilabel ===

# Accuracy per label
for i, col in enumerate(label_cols):
    acc = accuracy_score(y_test[:, i], y_pred_bin[:, i])
    print(f"Accuracy {col}: {acc:.4f}")

print("\nClassification Report (multilabel):")
print(classification_report(y_test, y_pred_bin, target_names=label_cols))

# ROC AUC per label
try:
    aucs = []
    for i in range(y_test.shape[1]):
        auc = roc_auc_score(y_test[:, i], y_pred_prob[:, i])
        aucs.append(auc)
        print(f"ROC AUC {label_cols[i]}: {auc:.4f}")
except Exception as e:
    print("ROC AUC warning:", str(e))

# === ROC Curve per label ===
for i, label in enumerate(mlb.classes_):
    fpr, tpr, thresh = roc_curve(y_test[:, i], y_pred_prob[:, i])
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {aucs[i]:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve: {label}')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()

# Confusion matrix per label
for i, col in enumerate(label_cols):
    cm = confusion_matrix(y_test[:, i], y_pred_bin[:, i])
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix {col}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model

# === Load tokenizer (word_index) ===
with open("aspect_tuning/word_index.json", "r") as f:
    word_index = json.load(f)
word_index = {str(k): int(v) for k, v in word_index.items()}

tokenizer = lambda text: [word_index.get(word, 0) for word in text.split()]

# === Load CNN model ===
model_cnn = load_model("aspect_tuning/model_final.h5")

# === Tokenisasi untuk Word2Vec ===
def tokenize_word2vec(texts, tokenizer, max_len=100):
    sequences = [tokenizer(text) for text in texts]
    return pad_sequences(sequences, maxlen=max_len, padding='post')

# === Test data ===
texts_test = df_test['stemmed_text']
label_cols=['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']
y_test = df_test[label_cols].values.astype(int)

mlb = MultiLabelBinarizer()
mlb.classes_ = label_cols


# === Preprocessing teks ===
X_test = tokenize_word2vec(texts_test, tokenizer, max_len=100)

# === Predict multilabel ===
y_pred_prob = model_cnn.predict(X_test)

# Threshold untuk multilabel (bisa disesuaikan)
thresholds = np.array([0.7, 0.5, 0.4, 0.4])
y_pred_bin = (y_pred_prob > 0.5).astype(int)

# === Evaluasi multilabel ===

# Accuracy per label
for i, col in enumerate(['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']):
    acc = accuracy_score(y_test[:, i], y_pred_bin[:, i])
    print(f"Accuracy {col}: {acc:.4f}")

print("\nClassification Report (multilabel):")
print(classification_report(y_test, y_pred_bin, target_names=['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']))

# ROC AUC per label
try:
    from sklearn.metrics import roc_auc_score
    aucs = []
    for i in range(y_test.shape[1]):
        auc = roc_auc_score(y_test[:, i], y_pred_prob[:, i])
        aucs.append(auc)
        print(f"ROC AUC {label_cols[i]}: {auc:.4f}")
except Exception as e:
    print("ROC AUC warning:", str(e))

from sklearn.metrics import roc_curve


# Confusion matrix per label (contoh untuk satu label, bisa loop sesuai kebutuhan)
import matplotlib.pyplot as plt
import seaborn as sns
for i, col in enumerate(['aspect_attraction', 'aspect_amenities', 'aspect_access', 'aspect_price']):
    cm = confusion_matrix(y_test[:, i], y_pred_bin[:, i])
    plt.figure(figsize=(4,3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix {col}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

"""##attraction"""

from transformers import DistilBertTokenizer
import tensorflow as tf
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel
from tensorflow.keras.models import load_model

# === LOAD TOKENIZER & MODEL BERT ===
tokenizer = DistilBertTokenizer.from_pretrained("attraction_tuning/tokenizer_final")
model_bert = TFDistilBertModel.from_pretrained("attraction_tuning/model_bert")

# === LOAD CNN CLASSIFIER ===
model_cnn = load_model("attraction_tuning/model_final.h5")
model_cnn.summary()

# === FUNCTION TO ENCODE TEST TEXTS ===
def bert_encode(texts, tokenizer, model_bert, max_len=100, batch_size=16):
    outputs_all = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        encodings = tokenizer(
            batch_texts.tolist(),
            truncation=True,
            padding='max_length',
            max_length=max_len,
            return_tensors="tf"
        )
        outputs = model_bert(encodings['input_ids'], attention_mask=encodings['attention_mask'])
        last_hidden = outputs.last_hidden_state  # shape: (batch_size, max_len, hidden_size)
        outputs_all.append(last_hidden.numpy())

    return np.vstack(outputs_all)

# Data Test (df_test) untuk diuji
texts_test = df_test_attraction['cleaned_text']  # Ganti sesuai dengan kolom teks pada df_test
y_test = df_test_attraction['sentiment_attraction'].astype(int)  # Ganti dengan kolom label yang sesuai

# Tokenisasi seluruh data test
#encoding_test = tokenizer(texts_test.tolist(), return_tensors="tf", padding=True, truncation=True, max_length=100)

# === ENCODE TEXTS ===
X_test = bert_encode(texts_test, tokenizer, model_bert, max_len=100)

# Prediksi dengan model
y_pred = model_cnn.predict(X_test)

# Mengubah prediksi ke biner (untuk binary classification)
y_pred_bin = (y_pred > 0.75).astype(int)

# Evaluasi: Accuracy, Precision, Recall, F1 score
accuracy = accuracy_score(y_test, y_pred_bin)
print(f"Accuracy: {accuracy:.4f}")

# Classification report untuk evaluasi lebih mendalam
print("\nClassification Report:")
print(classification_report(y_test, y_pred_bin, target_names=['Negatif', 'Positif']))

# === ROC AUC Score (menggunakan probabilitas)
try:
    roc_auc = roc_auc_score(y_test, y_pred)  # Menggunakan probabilitas, bukan y_pred_bin
    print(f"ROC AUC: {roc_auc:.4f}")
except ValueError as e:
    print("ROC AUC warning:", str(e))

# === Confusion Matrix
cm = confusion_matrix(y_test, y_pred_bin)
plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# === ROC Curve (menggunakan probabilitas untuk ROC Curve)
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

plt.figure()
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""##amenities"""

from transformers import DistilBertTokenizer
import tensorflow as tf
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel
from tensorflow.keras.models import load_model

# === LOAD TOKENIZER & MODEL BERT ===
tokenizer = DistilBertTokenizer.from_pretrained("amenities_tuning/tokenizer_final")
model_bert = TFDistilBertModel.from_pretrained("amenities_tuning/model_bert")

# === LOAD CNN CLASSIFIER ===
model_cnn = load_model("amenities_tuning/model_final.h5")
model_cnn.summary()

# === FUNCTION TO ENCODE TEST TEXTS ===
def bert_encode(texts, tokenizer, model_bert, max_len=100, batch_size=16):
    outputs_all = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        encodings = tokenizer(
            batch_texts.tolist(),
            truncation=True,
            padding='max_length',
            max_length=max_len,
            return_tensors="tf"
        )
        outputs = model_bert(encodings['input_ids'], attention_mask=encodings['attention_mask'])
        last_hidden = outputs.last_hidden_state  # shape: (batch_size, max_len, hidden_size)
        outputs_all.append(last_hidden.numpy())

    return np.vstack(outputs_all)

# Data Test (df_test) untuk diuji
texts_test = df_test_amenities['cleaned_text']  # Ganti sesuai dengan kolom teks pada df_test
y_test = df_test_amenities['sentiment_amenities'].astype(int)  # Ganti dengan kolom label yang sesuai

# Tokenisasi seluruh data test
#encoding_test = tokenizer(texts_test.tolist(), return_tensors="tf", padding=True, truncation=True, max_length=100)

# === ENCODE TEXTS ===
X_test = bert_encode(texts_test, tokenizer, model_bert, max_len=100)

# Prediksi dengan model
y_pred = model_cnn.predict(X_test)

# Mengubah prediksi ke biner (untuk binary classification)
y_pred_bin = (y_pred > 0.5).astype(int)

# Evaluasi: Accuracy, Precision, Recall, F1 score
accuracy = accuracy_score(y_test, y_pred_bin)
print(f"Accuracy: {accuracy:.4f}")

# Classification report untuk evaluasi lebih mendalam
print("\nClassification Report:")
print(classification_report(y_test, y_pred_bin, target_names=['Negatif', 'Positif']))

# === ROC AUC Score (menggunakan probabilitas)
try:
    roc_auc = roc_auc_score(y_test, y_pred)  # Menggunakan probabilitas, bukan y_pred_bin
    print(f"ROC AUC: {roc_auc:.4f}")
except ValueError as e:
    print("ROC AUC warning:", str(e))

# === Confusion Matrix
cm = confusion_matrix(y_test, y_pred_bin)
plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# === ROC Curve (menggunakan probabilitas untuk ROC Curve)
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

plt.figure()
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""##access"""

from transformers import DistilBertTokenizer
import tensorflow as tf
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel
from tensorflow.keras.models import load_model

# === LOAD TOKENIZER & MODEL BERT ===
tokenizer = DistilBertTokenizer.from_pretrained("access_tuning/tokenizer_final")
model_bert = TFDistilBertModel.from_pretrained("access_tuning/model_bert")

# === LOAD CNN CLASSIFIER ===
model_cnn = load_model("access_tuning/model_final.h5")
model_cnn.summary()

# === FUNCTION TO ENCODE TEST TEXTS ===
def bert_encode(texts, tokenizer, model_bert, max_len=100, batch_size=16):
    outputs_all = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        encodings = tokenizer(
            batch_texts.tolist(),
            truncation=True,
            padding='max_length',
            max_length=max_len,
            return_tensors="tf"
        )
        outputs = model_bert(encodings['input_ids'], attention_mask=encodings['attention_mask'])
        last_hidden = outputs.last_hidden_state  # shape: (batch_size, max_len, hidden_size)
        outputs_all.append(last_hidden.numpy())

    return np.vstack(outputs_all)

# Data Test (df_test) untuk diuji
texts_test = df_test_access['cleaned_text']  # Ganti sesuai dengan kolom teks pada df_test
y_test = df_test_access['sentiment_access'].astype(int)  # Ganti dengan kolom label yang sesuai

# Tokenisasi seluruh data test
#encoding_test = tokenizer(texts_test.tolist(), return_tensors="tf", padding=True, truncation=True, max_length=100)

# === ENCODE TEXTS ===
X_test = bert_encode(texts_test, tokenizer, model_bert, max_len=100)

# Prediksi dengan model
y_pred = model_cnn.predict(X_test)

# Mengubah prediksi ke biner (untuk binary classification)
y_pred_bin = (y_pred > 0.5).astype(int)

# Evaluasi: Accuracy, Precision, Recall, F1 score
accuracy = accuracy_score(y_test, y_pred_bin)
print(f"Accuracy: {accuracy:.4f}")

# Classification report untuk evaluasi lebih mendalam
print("\nClassification Report:")
print(classification_report(y_test, y_pred_bin, target_names=['Negatif', 'Positif']))

# === ROC AUC Score (menggunakan probabilitas)
try:
    roc_auc = roc_auc_score(y_test, y_pred)  # Menggunakan probabilitas, bukan y_pred_bin
    print(f"ROC AUC: {roc_auc:.4f}")
except ValueError as e:
    print("ROC AUC warning:", str(e))

# === Confusion Matrix
cm = confusion_matrix(y_test, y_pred_bin)
plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# === ROC Curve (menggunakan probabilitas untuk ROC Curve)
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

plt.figure()
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""##price"""

from transformers import DistilBertTokenizer
import tensorflow as tf
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel
from tensorflow.keras.models import load_model

# === LOAD TOKENIZER & MODEL BERT ===
tokenizer = DistilBertTokenizer.from_pretrained("price_tuning/tokenizer_final")
model_bert = TFDistilBertModel.from_pretrained("price_tuning/model_bert")

# === LOAD CNN CLASSIFIER ===
model_cnn = load_model("price_tuning/model_final.h5")
model_cnn.summary()

# === FUNCTION TO ENCODE TEST TEXTS ===
def bert_encode(texts, tokenizer, model_bert, max_len=100, batch_size=16):
    outputs_all = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        encodings = tokenizer(
            batch_texts.tolist(),
            truncation=True,
            padding='max_length',
            max_length=max_len,
            return_tensors="tf"
        )
        outputs = model_bert(encodings['input_ids'], attention_mask=encodings['attention_mask'])
        last_hidden = outputs.last_hidden_state  # shape: (batch_size, max_len, hidden_size)
        outputs_all.append(last_hidden.numpy())

    return np.vstack(outputs_all)

# Data Test (df_test) untuk diuji
texts_test = df_test_price['cleaned_text']  # Ganti sesuai dengan kolom teks pada df_test
y_test = df_test_price['sentiment_price'].astype(int)  # Ganti dengan kolom label yang sesuai

# Tokenisasi seluruh data test
#encoding_test = tokenizer(texts_test.tolist(), return_tensors="tf", padding=True, truncation=True, max_length=100)

# === ENCODE TEXTS ===
X_test = bert_encode(texts_test, tokenizer, model_bert, max_len=100)

# Prediksi dengan model
y_pred = model_cnn.predict(X_test)

# Mengubah prediksi ke biner (untuk binary classification)
y_pred_bin = (y_pred > 0.5).astype(int)

# Evaluasi: Accuracy, Precision, Recall, F1 score
accuracy = accuracy_score(y_test, y_pred_bin)
print(f"Accuracy: {accuracy:.4f}")

# Classification report untuk evaluasi lebih mendalam
print("\nClassification Report:")
print(classification_report(y_test, y_pred_bin, target_names=['Negatif', 'Positif']))

# === ROC AUC Score (menggunakan probabilitas)
try:
    roc_auc = roc_auc_score(y_test, y_pred)  # Menggunakan probabilitas, bukan y_pred_bin
    print(f"ROC AUC: {roc_auc:.4f}")
except ValueError as e:
    print("ROC AUC warning:", str(e))

# === Confusion Matrix
cm = confusion_matrix(y_test, y_pred_bin)
plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# === ROC Curve (menggunakan probabilitas untuk ROC Curve)
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

plt.figure()
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""#Download"""

# prompt: download folder

!zip -r /content/aspect_tuning.zip /content/aspect_tuning/
from google.colab import files
files.download('/content/aspect_tuning.zip')